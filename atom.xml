<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Gugugu&#39;s blog</title>
  
  
  <link href="https://luoyongjia.github.io/atom.xml" rel="self"/>
  
  <link href="https://luoyongjia.github.io/"/>
  <updated>2021-05-07T03:56:51.377Z</updated>
  <id>https://luoyongjia.github.io/</id>
  
  <author>
    <name>罗咏佳</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【ML】Self-attention</title>
    <link href="https://luoyongjia.github.io/2021/05/07/%E3%80%90ML%E3%80%91Self-attention/"/>
    <id>https://luoyongjia.github.io/2021/05/07/%E3%80%90ML%E3%80%91Self-attention/</id>
    <published>2021-05-07T02:53:21.000Z</published>
    <updated>2021-05-07T03:56:51.377Z</updated>
    
    <content type="html"><![CDATA[<p>在2020课程看到RNN的时候，突然在旁边发现了2021的一个讲transformer的，诶嘿，就找到2021的课程了。大约之前的都是一样的，所以就直接从cnn之后的self-attention开始了。</p><h1 id="适用的任务"><a href="#适用的任务" class="headerlink" title="适用的任务"></a>适用的任务</h1><h2 id="大致框架"><a href="#大致框架" class="headerlink" title="大致框架"></a>大致框架</h2><p><strong>输入</strong>的是一个序列的vector，不同的序列之间很可能长短不一。<strong>输出</strong>可以是和输入的vector数量相同，也可以是不同的。</p><h2 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h2><p>文字输入，判断每个词的词性。<br>分子输入（图），判断这个分子的有效性。</p><p>可以是一段音频，也可以是一个图，只要是vector序列，就可以作为输入。</p><h1 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h1><p>比如说输入一句话：I saw a saw.<br>这里两个saw分别代表不同的词性。但是就之前的神经网络来说，这两个输入是相同的，所以输出也应该是相同的。就不对了。这里就需要结合更多的相关信息（比如说前面是什么词性，比如说在整句中是一个什么位置）来判断这个saw是什么词性了。</p><h2 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h2><p>有一种方法是增加一个window，使得每次的输入包括的vector增加，增加一个前后的输入。</p><p><img src="/2021/05/07/%E3%80%90ML%E3%80%91Self-attention/1.png" alt="f1"></p><p>但是这种方法很明显存在一个缺陷，就是我的感受范围是确定的，也就是说没办法很好地覆盖每一个输入。</p><h2 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h2><p>它做了一个什么操作呢？假设我们这个序列里有<script type="math/tex">n</script>个vector, 然后通过self-attention后，每个vector都根据与其他vector的相关性进行了一定的融合，然后生成了一个新的vector，输出对应的<script type="math/tex">n</script>个vector。是不是很像当时logistic regression最后提到的，换一个分布。但是这里是根据全局的其他相关信息换的。从名字也可以知道，attention，就是关注更多与自己相关的输入。</p><p><img src="/2021/05/07/%E3%80%90ML%E3%80%91Self-attention/2.png" alt="f2"></p><p>这个self-attention可以在网络中多次出现，可以在不同位置出现。</p><h1 id="具体操作"><a href="#具体操作" class="headerlink" title="具体操作"></a>具体操作</h1><h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><p>假设我们现在想算<script type="math/tex">a^{1}</script>的输出。那么首先，我们需要计算出<script type="math/tex">a^{1}</script>与其他几个vector的相关度。这里采用Dot-product方法计算相关度。当然也存在其他的计算相关度的方法，仅是这里采用此方法。</p><p><img src="/2021/05/07/%E3%80%90ML%E3%80%91Self-attention/3.png" alt="f3"></p><p>第一步算出<script type="math/tex">a^{1}</script>与其他的相关度</p><p><img src="/2021/05/07/%E3%80%90ML%E3%80%91Self-attention/4.png" alt="f4"></p><p>然后过一层normalization层。</p><p><img src="/2021/05/07/%E3%80%90ML%E3%80%91Self-attention/5.png" alt="f5"></p><p>计算出每个vector对应的v之后与经过normalization的相关度相乘再相加。</p><p><img src="/2021/05/07/%E3%80%90ML%E3%80%91Self-attention/6.png" alt="f6"></p><p>就算<script type="math/tex">a^{1}</script>对应的输出<script type="math/tex">b^{1}</script>啦。由于每个v乘的系数是经过normalization的相关度，所以说再<script type="math/tex">b^{1}</script>中相关度高的影响就大，相关度低的影响较小，从而做到了关注相关的。</p><p>课上老师提出的：是否可以删除计算自己的相关度？我认为是不行的，因为我认为输出主要的部分还是原输入，如果抛弃了原输入只关注周围信息，肯定是没办法得到想要的结果的。</p><h2 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h2><p>对于每一个输入的向量，都做了上述的操作。那，理所当然，我们就想到了用矩阵加速。</p><p>对于每一个输入，我们都需要乘系数<script type="math/tex">W^{q}, W^{k}, W^{v}</script>，那么，就可以得到：</p><p><img src="/2021/05/07/%E3%80%90ML%E3%80%91Self-attention/7.png" alt="f7"></p><p>求相关度可以合并为：</p><p><img src="/2021/05/07/%E3%80%90ML%E3%80%91Self-attention/8.png" alt="f8"></p><p>最后再乘一个V：</p><p><img src="/2021/05/07/%E3%80%90ML%E3%80%91Self-attention/9.png" alt="f10"></p><p>也就是说，这个运算最后可以简化为：<script type="math/tex">O = VK^{T}Q</script>这样子。</p><h2 id="multi-head-self-attention"><a href="#multi-head-self-attention" class="headerlink" title="multi-head self-attention"></a>multi-head self-attention</h2><p>上面的都是基于一个head的。就称他们为一套吧。我现在可以有多套，然后出来多个b，最后乘个系数得到唯一输出b。就是这样子。</p><p><img src="/2021/05/07/%E3%80%90ML%E3%80%91Self-attention/10.png" alt="f10"></p><h2 id="位置信息"><a href="#位置信息" class="headerlink" title="位置信息"></a>位置信息</h2><p>从上面的操作我们可以看出，这么一操作，位置信息就全咩了。所以如果需要位置信息，我们可以给每个输入加上一个位置向量。这个向量可以是学出来的，也可以是手动设置的。当然，这个还在研究中这样子。</p><p><img src="/2021/05/07/%E3%80%90ML%E3%80%91Self-attention/11.png" alt="f11"></p><h1 id="一些应用"><a href="#一些应用" class="headerlink" title="一些应用"></a>一些应用</h1><h2 id="Speech"><a href="#Speech" class="headerlink" title="Speech"></a>Speech</h2><p>语音任务，一段语音特别长，我们可以把attention的范围缩小到一定范围中</p><h2 id="Image"><a href="#Image" class="headerlink" title="Image"></a>Image</h2><p>就是把每个点的不同通道数值作为一个vector，整个图片就有很多vector了。（这样是不是就可以把不同大小的图片作为输入了？）</p><p><img src="/2021/05/07/%E3%80%90ML%E3%80%91Self-attention/12.png" alt="f12"></p><h2 id="Self-attention-v-s-CNN"><a href="#Self-attention-v-s-CNN" class="headerlink" title="Self-attention v.s. CNN"></a>Self-attention v.s. CNN</h2><p>CNN像是有很多先验的self-attention。如果数据集较小可以选择CNN，大的数据选Self-attention</p><p><img src="/2021/05/07/%E3%80%90ML%E3%80%91Self-attention/13.png" alt="f13"></p><h2 id="Self-attention-v-s-RNN"><a href="#Self-attention-v-s-RNN" class="headerlink" title="Self-attention v.s. RNN"></a>Self-attention v.s. RNN</h2><p>RNN的范围扩大是通过储存之前的输出。这样子导致输出具有序列性。并且不能够并行。</p><h6 id="refer"><a href="#refer" class="headerlink" title="refer"></a>refer</h6><hr><p><a href="https://arxiv.org/pdf/2010.11929.pdf 37">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a><br><a href="https://www.youtube.com/watch?v=hYdO9CscNes">Attention 1</a><br><a href="https://www.youtube.com/watch?v=gmsMY5kc-zw">Attention 2</a>    </p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在2020课程看到RNN的时候，突然在旁边发现了2021的一个讲transformer的，诶嘿，就找到2021的课程了。大约之前的都是一样的，所以就直接从cnn之后的self-attention开始了。&lt;/p&gt;
&lt;h1 id=&quot;适用的任务&quot;&gt;&lt;a href=&quot;#适用的任务&quot;</summary>
      
    
    
    
    <category term="深度学习基础" scheme="https://luoyongjia.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
  </entry>
  
  <entry>
    <title>Colab使用基础指南</title>
    <link href="https://luoyongjia.github.io/2021/05/05/Colab%E4%BD%BF%E7%94%A8%E5%9F%BA%E7%A1%80%E6%8C%87%E5%8D%97/"/>
    <id>https://luoyongjia.github.io/2021/05/05/Colab%E4%BD%BF%E7%94%A8%E5%9F%BA%E7%A1%80%E6%8C%87%E5%8D%97/</id>
    <published>2021-05-05T11:57:50.000Z</published>
    <updated>2021-05-05T12:32:35.341Z</updated>
    
    <content type="html"><![CDATA[<p>嫖嫖嫖，就硬嫖。香香香！白嫖的显卡就是香！</p><p>这里来几个基础操作。</p><h1 id="IO"><a href="#IO" class="headerlink" title="IO"></a>IO</h1><p>可以看到，Colab是基于Google Drive的。要读取我们云端的数据，就需要：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from google.colab import drive</span><br><span class="line">drive.mount(&#39;&#x2F;content&#x2F;drive&#x2F;&#39;)</span><br><span class="line"># 设置工作路径，个人觉得可以咩</span><br><span class="line">import os</span><br><span class="line">os.chdir(&quot;&#x2F;content&#x2F;drive&#x2F;My Drive&#x2F;YourWorkPath&quot;)</span><br></pre></td></tr></table></figure><p>这是每一个ipynb文件都需要先执行一下下得。</p><h2 id="读取文件"><a href="#读取文件" class="headerlink" title="读取文件"></a>读取文件</h2><p>就是路径读取。我们先将文件上传到云上面。然后在Colab的左侧，文件那一栏，就可以看到我们的文件都路径。通过路径读取就ok。</p><p><img src="/2021/05/05/Colab%E4%BD%BF%E7%94%A8%E5%9F%BA%E7%A1%80%E6%8C%87%E5%8D%97/1.png" alt="f1"></p><h1 id="shell"><a href="#shell" class="headerlink" title="shell"></a>shell</h1><p>查看已安装的包: <code>! pip list</code><br>安装新的包： <code>! pip install packegeName</code><br>查看嫖到什么显卡：<code>! /opt/bin/nvidia-smi</code><br>进入bash: <code>! bash</code><br>推出：<code>exit</code></p><h1 id="保持连接"><a href="#保持连接" class="headerlink" title="保持连接"></a>保持连接</h1><p>没开pro最长就是12h，不够用了再说吧。如何保持12h不断开？<br>F12看网页代码，打开<strong>console</strong>输入这行代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">function ClickConnect()&#123;</span><br><span class="line">    console.log(&quot;Clicked on connect button&quot;); </span><br><span class="line">    document.querySelector(&quot;colab-connect-button&quot;).click()</span><br><span class="line">&#125;</span><br><span class="line">setInterval(ClickConnect,60000)</span><br></pre></td></tr></table></figure><p>每60s按一次colab-connecte-button，保持连接。<br>每次刷新之后需要再次输入。</p><h1 id="常用快捷键"><a href="#常用快捷键" class="headerlink" title="常用快捷键"></a>常用快捷键</h1><p><img src="/2021/05/05/Colab%E4%BD%BF%E7%94%A8%E5%9F%BA%E7%A1%80%E6%8C%87%E5%8D%97/2.png" alt="f2"><br><img src="/2021/05/05/Colab%E4%BD%BF%E7%94%A8%E5%9F%BA%E7%A1%80%E6%8C%87%E5%8D%97/3.png" alt="f3">    </p><h6 id="refer"><a href="#refer" class="headerlink" title="refer"></a>refer</h6><hr><p><a href="http://www.zhaoyongjie.cn/post/487.html">Google Colab入门（一）</a><br><a href="https://www.wxp123.me/p/197">Google Colab 如何查看显卡配置和驱动情况？</a><br><a href="https://zhuanlan.zhihu.com/p/144629818?from_voters_page=true">colab保持连接的方法</a><br><a href="https://www.jiqizhixin.com/articles/2020-09-27-2">20种小技巧，玩转Google Colab</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;嫖嫖嫖，就硬嫖。香香香！白嫖的显卡就是香！&lt;/p&gt;
&lt;p&gt;这里来几个基础操作。&lt;/p&gt;
&lt;h1 id=&quot;IO&quot;&gt;&lt;a href=&quot;#IO&quot; class=&quot;headerlink&quot; title=&quot;IO&quot;&gt;&lt;/a&gt;IO&lt;/h1&gt;&lt;p&gt;可以看到，Colab是基于Google Dri</summary>
      
    
    
    
    <category term="基本操作" scheme="https://luoyongjia.github.io/categories/%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    
    
    <category term="Colab" scheme="https://luoyongjia.github.io/tags/Colab/"/>
    
  </entry>
  
  <entry>
    <title>【ML】CNN</title>
    <link href="https://luoyongjia.github.io/2021/04/23/%E3%80%90ML%E3%80%91CNN/"/>
    <id>https://luoyongjia.github.io/2021/04/23/%E3%80%90ML%E3%80%91CNN/</id>
    <published>2021-04-23T13:54:40.000Z</published>
    <updated>2021-04-25T12:00:04.216Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅机器学习笔记</p><h1 id="图像的特点"><a href="#图像的特点" class="headerlink" title="图像的特点"></a>图像的特点</h1><ol><li>图像的特征很小，比如说鸟嘴，鸟尾巴，鸟眼睛这种，对于一整张图片来说，这知识一部分</li><li>同一个特征可能出现在一张图片的不同位置</li><li>subsampling，一个图片放大缩小旋转这些操作，并不会影响特征的提取</li></ol><p>所以，可以使用卷积来提取特征。</p><h1 id="CNN的整个框架"><a href="#CNN的整个框架" class="headerlink" title="CNN的整个框架"></a>CNN的整个框架</h1><p><img src="/2021/04/23/%E3%80%90ML%E3%80%91CNN/1.png" alt="f1"></p><p>卷积操作可以对应图像的特征1和特征2。卷积核可以在不同位置识别出较小的特征。</p><p>maxPooling对应图片缩小并不会影响判断的特征。</p><p>然后这个过程可以重复好多遍，从底层的特征提取到高层的特征。</p><h1 id="Convolution（基础卷积）"><a href="#Convolution（基础卷积）" class="headerlink" title="Convolution（基础卷积）"></a>Convolution（基础卷积）</h1><p>就是使用多个卷积核，在输入的图片上进行卷积操作。需要注意到：卷积核的channel数应该和输入的channel数相同，卷积核的个数对应输出的channel数。具体怎么算我就不赘述了，我的汇报ppt上有，并且还有各种奇奇怪怪的卷积。以及对应的参数，功能，时间、空间复杂度的计算，复习复习就行。</p><p>就是对应位置相乘，然后取和。输出就是：Feature Map</p><p>注意卷积计算，filter的channel数和输入的channel数是一样的，输出channel数等于filter的个数。</p><h2 id="conv和全连接的区别"><a href="#conv和全连接的区别" class="headerlink" title="conv和全连接的区别"></a>conv和全连接的区别</h2><p>老师课程中说得非常清楚，这个图很好的表现了conv和全连接之间的区别。</p><p><img src="/2021/04/23/%E3%80%90ML%E3%80%91CNN/2.png" alt="f2"></p><p>其实就是消除了全连接之中的一些我们根据图像特征判断出的无效的连接，并且实现了一些参数共享，从而减少了参数量。</p><h1 id="max-pooling"><a href="#max-pooling" class="headerlink" title="max pooling"></a>max pooling</h1><p>就是对于图像进行一个缩小。从而减少计算量。主要操作有max，或是average。一般为变长为2的，就是直接缩少到一半。</p><h1 id="Flatten"><a href="#Flatten" class="headerlink" title="Flatten"></a>Flatten</h1><p>将所有的输出拉长，然后进行分类这样子。</p><h1 id="CNN在干什么？"><a href="#CNN在干什么？" class="headerlink" title="CNN在干什么？"></a>CNN在干什么？</h1><p>如何去分析一个filter在干嘛？</p><p>先确定这个函数被激活的程度：</p><script type="math/tex; mode=display">a^{k} = \sum_{i = 1}^{11} \sum_{j = 1}^{11} a_{ij}^{k}</script><p>然后我们将这个作为指标，来设计loss：</p><script type="math/tex; mode=display">x* = arg \substack{\max\\x} a^{k}</script><p>然后通过gradient ascent来求这个<script type="math/tex">x*</script></p><p>底层的大多去学一个简单的特征，比如说线啊，重复的波纹啊。然后对于较高的层，我们能够识别比较复杂的特征，比如说是一只猫，还是一只狗，它们特征集合的这样子。</p><h1 id="CNN的应用"><a href="#CNN的应用" class="headerlink" title="CNN的应用"></a>CNN的应用</h1><p>我们可以根据任务来设计我们的CNN模型。CNN不仅仅可以应用于图片，还可以应用于alpha go。</p><p>但是从alpha go的任务出发，我们可以发现，棋谱并没有那个可以缩小放大的性能。所以在alpha go的模型中可以发现，并没有pooling层。</p><p>其他两个例子我也没有太理解。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅机器学习笔记&lt;/p&gt;
&lt;h1 id=&quot;图像的特点&quot;&gt;&lt;a href=&quot;#图像的特点&quot; class=&quot;headerlink&quot; title=&quot;图像的特点&quot;&gt;&lt;/a&gt;图像的特点&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;图像的特征很小，比如说鸟嘴，鸟尾巴，鸟眼睛这种，对于一整张图片来说，这</summary>
      
    
    
    
    <category term="深度学习基础" scheme="https://luoyongjia.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
  </entry>
  
  <entry>
    <title>【ML】How to train DNN</title>
    <link href="https://luoyongjia.github.io/2021/04/23/%E3%80%90ML%E3%80%91How-to-train-DNN/"/>
    <id>https://luoyongjia.github.io/2021/04/23/%E3%80%90ML%E3%80%91How-to-train-DNN/</id>
    <published>2021-04-23T07:14:54.000Z</published>
    <updated>2021-04-25T08:43:57.973Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅机器学习笔记<br>袜，这课真的，老师讲得超好！👍👍👍超推荐去看！</p><h1 id="Process"><a href="#Process" class="headerlink" title="Process"></a>Process</h1><p>要了解如何训练自己的深度网络模型，第一步需要了解训练的整个过程。</p><p><img src="/2021/04/23/%E3%80%90ML%E3%80%91How-to-train-DNN/1.png" alt="f1"></p><p>从图中可以看出，在最初的<strong>设置模型</strong>，<strong>设置loss</strong>，<strong>解loss</strong>了之后，我们需要现在训练集上测试我们这个模型的效果。如果效果还行，我们就可以使用测试集的数据来测试模型的效果。</p><p>如果在训练集上我们的模型效果一直都不行，那这样肯定是左侧的三个步骤中某个出问题了。</p><p>当在训练集效果还行，但是在测试集效果不怎么行，则很可能是出现了过拟合的问题。</p><h1 id="左侧三步调整"><a href="#左侧三步调整" class="headerlink" title="左侧三步调整"></a>左侧三步调整</h1><p>也就是对于我们建的模进行调整。在training set上都没效果，还对于testing set做什么奢望？</p><h2 id="Activation-function"><a href="#Activation-function" class="headerlink" title="Activation function"></a>Activation function</h2><p>给予网络一种非线性的关系。</p><h3 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h3><p>当网络很深的时候，因为每一个sigmoid都会将输出控制到0-1之间，所以我们可以预见性的看见，当网络非常深的时候，接近输入层的参数对于结果的影响会被变得非常小，这时候对于接近输入层的参数会发生梯度小时的问题。当接近输出层的参数已经comverge了的时候，接近输入层的参数可能还是原来随机初始化的那几个，所以，这种激活函数根本不合适较深的网络。</p><p>我们如何去估计这种情况呢？我们可以知道，参数的更新是和<script type="math/tex">\frac{\partial l}{\partial w}</script>直接相关的，所以我们只需要看<script type="math/tex">\Delta l</script>与<script type="math/tex">\Delta w</script>的关系就👌啦。<br>比如说sigmoid函数，我们就可以看出，当网络深的时候，接近输出层的<script type="math/tex">\frac{\partial l}{\partial w}</script>大，接近输入层的<script type="math/tex">\frac{\partial l}{\partial w}</script>小。所以，网络深了接近输入的<script type="math/tex">w</script>根本更新不了输入层的参数，就导致网络效果不好。这时候，有一个解决办法就是换激活函数。</p><p><img src="/2021/04/23/%E3%80%90ML%E3%80%91How-to-train-DNN/2.png" alt="f2"></p><h3 id="ReLu"><a href="#ReLu" class="headerlink" title="ReLu"></a>ReLu</h3><h4 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h4><p><img src="/2021/04/23/%E3%80%90ML%E3%80%91How-to-train-DNN/3.png" alt="f3"></p><p>这东西好多理由的，真是，有理有据。<br>在一些输出小于0的神经元，等于直接把它们给删了，然后训练一个简单的框架。多次训练，删不同的神经元，从而使得所有的神经元都得到了训练。并且，对于留下的神经元，是一个线形的关系，就不存在sigmoid的梯度消失的问题。（多个不同的线形关系组合从而给了网络非线性关系。）</p><p>回传的时候求导，我们默认无0点。所以非1即0。</p><h4 id="几种变种"><a href="#几种变种" class="headerlink" title="几种变种"></a>几种变种</h4><p><strong>Leaky ReLu</strong>: ReLu中负数部分用0.01*output表示</p><p><img src="/2021/04/23/%E3%80%90ML%E3%80%91How-to-train-DNN/4.png" alt="f4"></p><p><strong>Parametric ReLu</strong>: 负数部分作为一个参数来学习</p><p><img src="/2021/04/23/%E3%80%90ML%E3%80%91How-to-train-DNN/5.png" alt="f5"></p><h3 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h3><p>学习激活函数（但是不知道为什么，我总觉着这东西不怎么靠谱，这也要学，neural的参数也要学，那岂不是很慢。）</p><p><img src="/2021/04/23/%E3%80%90ML%E3%80%91How-to-train-DNN/6.png" alt="f6"></p><p>它的过程大约是这样: 输入，通过线形变化了之后，在预先设置的群组中选择最大值作为输出。然后每一层都这么整。<br>通过这种方法是可以学出ReLu的，比如说将两个输出，其中一个恒等于0</p><p><img src="/2021/04/23/%E3%80%90ML%E3%80%91How-to-train-DNN/7.png" alt="f7"></p><p>其中红色和蓝色是两个输出，然后学出来就是绿色的，就跟ReLu是一个样子了。</p><p>通过这种方法可以根据不同的数据集、自己设置的群组的不同，学习出各种各样的激活函数。有效性我没有验证，但是有一篇paper(<a href="https://arxiv.org/abs/1906.09529">Learning Activation Functions</a>)，感兴趣的童鞋可以去康康。</p><p>大约，可以学出各种各样的激活函数：</p><p><img src="/2021/04/23/%E3%80%90ML%E3%80%91How-to-train-DNN/8.png" alt="f8"></p><h2 id="optimizer"><a href="#optimizer" class="headerlink" title="optimizer"></a>optimizer</h2><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>之前在GD中就已经介绍了Adagrad，详情可以移步<a href="https://luoyongjia.github.io/2021/04/12/%E3%80%90ML%E3%80%91Gradient-Descent/">【ML】Gradient Descent</a>。这里简单介绍一下，大约就是对于梯度下降中所有之前的梯度求一个均方根，然后让现梯度乘learning rate，再除这个均方根。</p><h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><p>是对Adagrad的一个升级版本。可以调整过去的梯度对于当前梯度的影响大小。</p><script type="math/tex; mode=display">\begin{equation*}\begin{aligned}&w^{t + 1} \gets w^{t} - \frac{\eta}{\sigma^{t}}g^{t} \\&\sigma^{t} = \sqrt{\alpha(\sigma^{t-1})^{2} + (1 - \alpha)(g^{t})^{2}}\\\end{aligned}\end{equation*}</script><p>所以说是可以通过调整<script type="math/tex">\alpha</script>的大小，来对于当前的learning rate进行更新，从而适应更加复杂的loss问题。相当于增加了一个衰减系数来控制历史信息的获取多少。</p><p>就，更加关注最近的几次梯度下降。那么如果当前参数空间比较平缓，那么这个均方和较小，那么就能走得更快。</p><p><img src="/2021/04/23/%E3%80%90ML%E3%80%91How-to-train-DNN/9.png" alt="f9"></p><h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>动量的方法。考虑前一次梯度下降的方向。过程大约是：</p><ol><li>初始化参数，和动能</li><li>计算参数所在点的梯度</li><li>动能 = <script type="math/tex">\lambda</script>上一次移动 - <script type="math/tex">\eta</script>梯度</li></ol><p><img src="/2021/04/23/%E3%80%90ML%E3%80%91How-to-train-DNN/10.png" alt="f10"></p><p>有的时候，可能之前的动能能够帮助它走出局部最优（当然，局部最优是很难出现的）</p><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>就是Monmentum和RMSProp的一个结合。算法长这样：</p><p><img src="/2021/04/23/%E3%80%90ML%E3%80%91How-to-train-DNN/11.png" alt="f11"></p><h1 id="过拟合调整"><a href="#过拟合调整" class="headerlink" title="过拟合调整"></a>过拟合调整</h1><h2 id="Early-stopping"><a href="#Early-stopping" class="headerlink" title="Early stopping"></a>Early stopping</h2><p>很好理解，我们就提前结束训练。简单、粗暴的方法。通常我们用验证集来验证我们是否过拟合。看它错误率上去了的时候，就cut。</p><h2 id="regularization"><a href="#regularization" class="headerlink" title="regularization"></a>regularization</h2><p>在loss后面添加二范式，从而达到，使我们拟合出来的函数更加平滑。</p><h3 id="范式"><a href="#范式" class="headerlink" title="范式"></a>范式</h3><h4 id="一范"><a href="#一范" class="headerlink" title="一范"></a>一范</h4><script type="math/tex; mode=display">\| \theta \|_{1} = |w_{1})| + |w_{2}| + \cdots</script><h4 id="二范"><a href="#二范" class="headerlink" title="二范"></a>二范</h4><script type="math/tex; mode=display">\| \theta \|_{2} = (w_{1})^{2} + (w_{2})^{2} + \cdots</script><h3 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h3><p>一范：<script type="math/tex">w^{t+1} \gets w^{t} - \eta \frac{\partial L}{\partial w} - \eta\lambda sgn(w^{t})</script><br>二范：<script type="math/tex">w^{t+1} \gets (1-\eta\lambda)w^{t} - \eta \frac{\partial L}{\partial w}</script></p><p>通过公式可以看出来，一范对于较大的参数没有办法很快的变小，但是二范能够做到，所以常用的还是二范，一范没什么用。</p><h2 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h2><p>假设每个神经元被丢掉的可能性是p%。每次训练中，随机的丢掉一些神经元，然后训练。这样，就形成了不同的网络结构。这些结构都被训练了。等于说，我用较少的神经元想要实现较多神经元要实现的任务。打个比方就像是负重训练一样。</p><p>然后在test的时候，将所有的神经元都启用，并且输出需要乘以（1-p%）因为我训练的时候神经元的数量相较于现在少p%，它们的输出就会被训练得放大，所以这里要恢复常值。</p><p>等于说，我训练了多个网络结构来做这一个task，然后我对于这些网络的输出做一个求和，诶嘿，刚好就等于（1-p%）所有神经元都用起来的网络的输出。</p><p><img src="/2021/04/23/%E3%80%90ML%E3%80%91How-to-train-DNN/12.png" alt="f12"></p><p>这里有个思考，这种方法和NAS的区别。会不会是NAS是对于n个网络结构训练了之后取效果最好的，但是这里直接就做平均了？暂时还不是特别清楚，以后再补充。</p><h6 id="refer"><a href="#refer" class="headerlink" title="refer"></a>refer</h6><hr><p><a href="https://www.youtube.com/watch?v=xki61j7z-30">ML Lecture 9-1: Tips for Training DNN</a><br><a href="https://zhuanlan.zhihu.com/p/34230849">【优化算法】一文搞懂RMSProp优化算法</a><br><a href="https://arxiv.org/pdf/1412.6980.pdf">Adam的paper</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅机器学习笔记&lt;br&gt;袜，这课真的，老师讲得超好！👍👍👍超推荐去看！&lt;/p&gt;
&lt;h1 id=&quot;Process&quot;&gt;&lt;a href=&quot;#Process&quot; class=&quot;headerlink&quot; title=&quot;Process&quot;&gt;&lt;/a&gt;Process&lt;/h1&gt;&lt;p&gt;要了解</summary>
      
    
    
    
    <category term="深度学习基础" scheme="https://luoyongjia.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
  </entry>
  
  <entry>
    <title>【ML】DNN</title>
    <link href="https://luoyongjia.github.io/2021/04/21/%E3%80%90ML%E3%80%91DNN/"/>
    <id>https://luoyongjia.github.io/2021/04/21/%E3%80%90ML%E3%80%91DNN/</id>
    <published>2021-04-21T13:19:14.000Z</published>
    <updated>2021-04-23T07:13:01.730Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅机器学习笔记</p><h1 id="发展史"><a href="#发展史" class="headerlink" title="发展史"></a>发展史</h1><p>1958: Preceptron(线形模型)<br>1980s: Multi-layer perceptorn<br>1986: Backpropagation<br>1989: 1 hidden layer is “good enough”, why deep<br>2006: RBM initializaiton(breakthrough)<br>2009: GPU<br>2011: speech recognition<br>2012: win ILSVRC image competition</p><h1 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h1><p>与机器学习步骤相同，只不过第一步换成了设计一个网络结构</p><h1 id="Why-Deep"><a href="#Why-Deep" class="headerlink" title="Why Deep?"></a>Why Deep?</h1><p>有种模块化的意思。就像软件设计中，会有很多基础功能，然后基础功能一层层结合，实现更加复杂的功能这样子。</p><p>课程中举了一个例子：一个分类网络，需要分辨出这个人是长发女，长发男，短发女还是短发男。假设现在长发男的数据特别少，我们想要一层分辨出长发男就很难。但是如果我们将网络搭两层，就能第一层学习是男还是女，是长发还是短发；然后第二层再做排列组合细分这样子。</p><p>在图像方面，就比如第一层学习了图中是否有横线、斜线等，然后第二层学习是否有某个花纹，第三层学习是否有某个物体一部分…逐步提高识别的细粒度。</p><p>在语音方面，首先学习这个音是怎样发出的，然后学习这些音的组合，然后再学习内容这样子。</p><p>所以deep，能够让网络将复杂的任务分解，使本需要大量数据训练的函数，现在需要较少的数据，也能做到较好的效果。网络，就是一个大型特征提取器。也可以简单的端到端，这样，网络能够通过一定量的数据，自动地学到如何去模组化这个任务。</p><h6 id="refer"><a href="#refer" class="headerlink" title="refer"></a>refer</h6><hr><p><a href="https://www.youtube.com/watch?v=Dr-WRlEFefw">DL</a><br><a href="https://www.youtube.com/watch?v=XsC9byQkUH8">Why Deep</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅机器学习笔记&lt;/p&gt;
&lt;h1 id=&quot;发展史&quot;&gt;&lt;a href=&quot;#发展史&quot; class=&quot;headerlink&quot; title=&quot;发展史&quot;&gt;&lt;/a&gt;发展史&lt;/h1&gt;&lt;p&gt;1958: Preceptron(线形模型)&lt;br&gt;1980s: Multi-layer perc</summary>
      
    
    
    
    <category term="深度学习基础" scheme="https://luoyongjia.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
  </entry>
  
  <entry>
    <title>【ML】Classification and Logistic Regression</title>
    <link href="https://luoyongjia.github.io/2021/04/18/%E3%80%90ML%E3%80%91Classification-and-Logistic-Regression/"/>
    <id>https://luoyongjia.github.io/2021/04/18/%E3%80%90ML%E3%80%91Classification-and-Logistic-Regression/</id>
    <published>2021-04-18T05:45:22.000Z</published>
    <updated>2021-04-19T08:14:44.665Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅机器学习笔记</p><h1 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h1><p>classification嘛，很好理解，就是分类问题。它和回归问题的最大差别在于，回归问题出来的结果是连续的，分类问题的结果是离散的。课中通过概率大约介绍了这个分类的过程。</p><h1 id="How"><a href="#How" class="headerlink" title="How?"></a>How?</h1><p>Input: 一个个体的各个特征。<br>Output: 把这个体分到哪一类。</p><h2 id="假设一个二分类的问题"><a href="#假设一个二分类的问题" class="headerlink" title="假设一个二分类的问题"></a>假设一个二分类的问题</h2><h3 id="Model-set"><a href="#Model-set" class="headerlink" title="Model set"></a>Model set</h3><p>现在有一个数据过来啦，我们怎么判断它是哪一类呢？</p><script type="math/tex; mode=display">P(C_{1}|x) = \frac{P(x|C_{1})P(C_{1})}{P(x|C_{1})P(C_{1}) + P(x|C_{2})P(C_{2})}</script><p>1 <script type="math/tex">P(C_{1}\mid x)</script>：当前有一个x，那么它是<script type="math/tex">C_{1}</script>类的可能性,<br>2 <script type="math/tex">P(C_{n})</script>：class n在整个数据集中占多大比例,<br>3 <script type="math/tex">P(x\mid C_{n})</script>：在class n中取到x的概率,         </p><p>所以这两个参数都是已知的，现在未知的只有<script type="math/tex">P(x|C_{1})</script>。通常我们假设这个class是一种分布，然后把x的feature值代进去，就可以取出来在这个模型的前提下，取到x到的概率是多少。</p><h4 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h4><p>就是对于预测结果的一个放大。</p><p><img src="/2021/04/18/%E3%80%90ML%E3%80%91Classification-and-Logistic-Regression/1.png" alt="f1"></p><h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><p>然后根据这个公式计算出的某个类作为预测结果，对于正确的个数求和（我觉得求百分比能够更加直观的看出效果），来评估预测效果。因为这个正确率我们是想要越大越好，而loss，求极小值为普遍方法，所以我们在loss之前加个负号。这里使用交叉熵loss。</p><script type="math/tex; mode=display">L = -[ylog\hat{y} +(1 - y)log(1 - \hat{y})]</script><h3 id="Optimition"><a href="#Optimition" class="headerlink" title="Optimition"></a>Optimition</h3><p>反正，我们到最后可以把这个离散的关系化做</p><script type="math/tex; mode=display">\sigma(wx + b)</script><p>然后还是用梯度下降。且，<script type="math/tex">\sigma(z)</script>只是一种代表的激活函数，激活函数之后会有一个总结。</p><h2 id="logistic-Regression"><a href="#logistic-Regression" class="headerlink" title="logistic Regression"></a>logistic Regression</h2><p>从上文化简之后，可以看出来也就是一个<script type="math/tex">wx + b</script>，就和linear regression很像了。怎么构造非线形的关系呢，激活函数。这种方法就是逻辑回归，用于分类。逻辑回归相较于线性回归，模型，loss不同，优化方法差不多。</p><p><img src="/2021/04/18/%E3%80%90ML%E3%80%91Classification-and-Logistic-Regression/2.png" alt="f2"></p><p>关系可以大概表示为这样子，是不是，很眼熟？？对，你想得没错，就是神经元，就是神经网络里面的神经元。</p><h3 id="feature-Transformation"><a href="#feature-Transformation" class="headerlink" title="feature Transformation"></a>feature Transformation</h3><p>有一些数据，他们原本的分布无法很好地进行分类，这时候就需要对他们进行调整。可以通过这种方法：</p><p><img src="/2021/04/18/%E3%80%90ML%E3%80%91Classification-and-Logistic-Regression/3.png" alt="f3"></p><p>诶嘿，是不是有神经网络那味儿了。神经网络其实就是隐藏层做一个好的特征提取器，跟这个差不多。</p><h6 id="refer"><a href="#refer" class="headerlink" title="refer"></a>refer</h6><hr><p><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html">李宏毅DL/ML</a></p><p>袜，过分，我几个公式调整了好久，发现一行没办法以latex公式开头。暂时还不知道怎么才能用latex作为头。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅机器学习笔记&lt;/p&gt;
&lt;h1 id=&quot;是什么&quot;&gt;&lt;a href=&quot;#是什么&quot; class=&quot;headerlink&quot; title=&quot;是什么&quot;&gt;&lt;/a&gt;是什么&lt;/h1&gt;&lt;p&gt;classification嘛，很好理解，就是分类问题。它和回归问题的最大差别在于，回归问题出来的</summary>
      
    
    
    
    <category term="深度学习基础" scheme="https://luoyongjia.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
  </entry>
  
  <entry>
    <title>【ML】Gradient Descent</title>
    <link href="https://luoyongjia.github.io/2021/04/12/%E3%80%90ML%E3%80%91Gradient-Descent/"/>
    <id>https://luoyongjia.github.io/2021/04/12/%E3%80%90ML%E3%80%91Gradient-Descent/</id>
    <published>2021-04-12T13:39:07.000Z</published>
    <updated>2021-04-19T07:31:51.687Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅机器学习3-1笔记。</p><h1 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h1><p>是一种得到loss最优值的一种方法。</p><blockquote><ol><li>随机取一点</li><li>求这一点的梯度</li><li>根据梯度以及learning rate来更新参数<br><br>从而得到最小的loss==得到最佳的参数</li></ol></blockquote><h1 id="为什么有效"><a href="#为什么有效" class="headerlink" title="为什么有效"></a>为什么有效</h1><p>GD其实是通过取局部loss下降最多的点，来决定参数的变化方向的。</p><p><img src="/2021/04/12/%E3%80%90ML%E3%80%91Gradient-Descent/1.png" alt="f1"></p><h2 id="如何快速地取到这个局部最优呢？"><a href="#如何快速地取到这个局部最优呢？" class="headerlink" title="如何快速地取到这个局部最优呢？"></a>如何快速地取到这个局部最优呢？</h2><p>这时候就引入了泰勒公式，将一个复杂的函数在一个很小的范围内取：</p><script type="math/tex; mode=display">h(x) \approx h(x_{0}) + h'(x_{0})(x - x_{0})</script><p>假设u、v为loss对于两个参数的偏微分，则：</p><script type="math/tex; mode=display">L(\theta) \approx s + u(\theta_{1} - a) + v(\theta_{2} - b)</script><p>想要<script type="math/tex">L(\theta)</script>取得最小值，则如图所示<script type="math/tex">(\Delta \theta_{1}, \Delta \theta_{2})</script>取(u, v)反方向，最大值。所以Gradient Descent的过程是<script type="math/tex">\theta^{1}\gets \theta ^{0 } - \eta \Delta L</script></p><p><img src="/2021/04/12/%E3%80%90ML%E3%80%91Gradient-Descent/2.png" alt="f2"></p><h1 id="Gradient-Descent的升级版"><a href="#Gradient-Descent的升级版" class="headerlink" title="Gradient Descent的升级版"></a>Gradient Descent的升级版</h1><h2 id="基于Learning-Rate"><a href="#基于Learning-Rate" class="headerlink" title="基于Learning Rate"></a>基于Learning Rate</h2><p>Learning rate 这个东西，太大很可能得不到最优值，太小呢效率又低，所以选择一个正确的learning rate是十分重要的。</p><p><img src="/2021/04/12/%E3%80%90ML%E3%80%91Gradient-Descent/4.png" alt="f4"><img src="/2021/04/12/%E3%80%90ML%E3%80%91Gradient-Descent/3.png" alt="f3"></p><h3 id="Adaptive-Learning-Rate"><a href="#Adaptive-Learning-Rate" class="headerlink" title="Adaptive Learning Rate"></a>Adaptive Learning Rate</h3><p>这是基于随着梯度下降的进行，当前点距离最优解是越来越近的理论的。随着梯度下降的进行，learning rate随之减小，比如说像这样的learning rate：</p><script type="math/tex; mode=display">\eta^{t} = \frac{\eta}{\sqrt{n + 1}}</script><h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><script type="math/tex; mode=display">\begin{equation*}\begin{aligned}&w^{t+1} \gets w^{t} - \frac{n^{t}}{\sigma^{t}}\\&\sigma^{t} = \sqrt{\frac{1}{t + 1}\sum_{i = 0}^{t}(g^{i})^{2}}\\& \eta^{t} = \frac{\eta}{\sqrt{n + 1}}\\\end{aligned}\end{equation*}</script><p>其中<script type="math/tex">\sigma^{t}</script>是之前这个<script type="math/tex">w</script>的所有偏微分值的均方根。这样子当t较大的时候，<script type="math/tex">\sigma^{t}</script>就能十分接近二次微分从而做到对于不同的<script type="math/tex">w</script>有一个将不同参数的分布进行类似归一化的处理。使得不同的参数的取值距离最优点距离的估算都是一个单位的。</p><p>对于上述算式进行化简，得到：</p><script type="math/tex; mode=display">w^{t+1} \gets w^{t} - \frac{\eta}{\sqrt{\sum_{i = 0}^{t} (g^{i})^{2}}}</script><h2 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h2><p>传统的GD是对于所有的训练数据计算梯度，然后更新系数。SGD是从训练数据中随机的抽取几个数据来算梯度，立马更新。显然这种方法更加快，效果也不错。</p><p><img src="/2021/04/12/%E3%80%90ML%E3%80%91Gradient-Descent/5.png" alt="f5"></p><h2 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h2><p>当不同参数传入的特征值单位相差很大的时候，不同参数对于结果的影响就会出现偏差，这样是非常不好的，不仅仅会影响到参数的更新，也会影响到训练的效率以及结果。所以需要将不同的特征值进行归一化处理。</p><p><img src="/2021/04/12/%E3%80%90ML%E3%80%91Gradient-Descent/6.png" alt="f6"></p><p>将特征的值，通过：</p><script type="math/tex; mode=display">x_{i}^{r} \gets \frac{x_{i}^{r} - m_{i}}{\sigma^{2}}</script><p>其中<script type="math/tex">m_{i}</script>是所有<script type="math/tex">x_{i}</script>这个特征值的平均值，<script type="math/tex">\sigma^{2}</script>是方差。这样处理之后，此特征的分布平均值为0，方差为1，对于所有的feature都进行这样的处理了之后，就服从相同的分布了。</p><h6 id="refer"><a href="#refer" class="headerlink" title="refer"></a>refer</h6><hr><p><a href="https://www.youtube.com/watch?v=yKKNr-QKz2Q&amp;feature=youtu.be">ML Lecture 3-1: Gradient Descent</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅机器学习3-1笔记。&lt;/p&gt;
&lt;h1 id=&quot;是什么&quot;&gt;&lt;a href=&quot;#是什么&quot; class=&quot;headerlink&quot; title=&quot;是什么&quot;&gt;&lt;/a&gt;是什么&lt;/h1&gt;&lt;p&gt;是一种得到loss最优值的一种方法。&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;</summary>
      
    
    
    
    <category term="深度学习基础" scheme="https://luoyongjia.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
  </entry>
  
  <entry>
    <title>【ML】Error</title>
    <link href="https://luoyongjia.github.io/2021/04/10/%E3%80%90ML%E3%80%91Error/"/>
    <id>https://luoyongjia.github.io/2021/04/10/%E3%80%90ML%E3%80%91Error/</id>
    <published>2021-04-10T09:52:54.000Z</published>
    <updated>2021-04-11T14:02:36.949Z</updated>
    
    <content type="html"><![CDATA[<p>这节课主要是讲，如何调教Average error。为何出现这种情况以及如何针对性解决问题。</p><p>我们的目标是—-在testing data上获得更好的表现！！！</p><h1 id="Bias-and-Variance-of-Estimator"><a href="#Bias-and-Variance-of-Estimator" class="headerlink" title="Bias and Variance of Estimator"></a>Bias and Variance of Estimator</h1><h2 id="Estimator"><a href="#Estimator" class="headerlink" title="Estimator"></a>Estimator</h2><p>估算器是用来判断这一群<script type="math/tex">f^{*}</script>跟正确的<script type="math/tex">\widehat{f}</script>之间的差距。有两个变量，Bias+Variance。</p><h2 id="Bias"><a href="#Bias" class="headerlink" title="Bias"></a>Bias</h2><p>其实就是你定义的函数对于正确函数之间的偏差。比如说正确的是5次函数，但是我们设置的是1次函数，这样子就出现了bias, 不管怎么训练，都没法儿很好的拟合数据。</p><h2 id="Variance"><a href="#Variance" class="headerlink" title="Variance"></a>Variance</h2><p>这个是关于训练的数据是否足够。不同的训练出来的模型是否效果都差不多。如果效果相差很多，就是variance大，这时候就需要更多的数据来训练模型。</p><h2 id="总的来说"><a href="#总的来说" class="headerlink" title="总的来说"></a>总的来说</h2><p>我觉得这个图片能够很好的说明问题</p><p><img src="/2021/04/10/%E3%80%90ML%E3%80%91Error/1.png" alt="f1"></p><p>怎么训练都训练不出，无非两个问题：</p><ol><li>model的原型选的就有问题。</li><li>训练的数据不够。</li></ol><h1 id="如何快速、有效挑选model？"><a href="#如何快速、有效挑选model？" class="headerlink" title="如何快速、有效挑选model？"></a>如何快速、有效挑选model？</h1><p>如果说，我每一个model都用所有的训练数据去训练，这样需要耗费大量的资源。然后还有一个问题，就是我们现在训练出来的参数只是针对当先数据的最优，我们怎么去应对未知的数据呢？</p><h2 id="Validation-set"><a href="#Validation-set" class="headerlink" title="Validation set"></a>Validation set</h2><p>这时候，验证集就出现啦。我们将验证集作为验证参数的有效性的依据，然后用testing set来模拟真实情况下未知的数据。可以一分为二，也可以将数据分为三份，然后每次挑其中一个作为验证集。将验证集拿来检验当前的model的效果，最后挑选效果最好的model用全部已知数据来训练。</p><p><img src="/2021/04/10/%E3%80%90ML%E3%80%91Error/2.png" alt="f2"></p><p>私以为，这种方法能够有效降低bias。</p><h6 id="refer"><a href="#refer" class="headerlink" title="refer"></a>refer</h6><hr><p><a href="https://www.youtube.com/watch?v=D_S6y0Jm6dQ">Where does the error come from?</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这节课主要是讲，如何调教Average error。为何出现这种情况以及如何针对性解决问题。&lt;/p&gt;
&lt;p&gt;我们的目标是—-在testing data上获得更好的表现！！！&lt;/p&gt;
&lt;h1 id=&quot;Bias-and-Variance-of-Estimator&quot;&gt;&lt;a hre</summary>
      
    
    
    
    <category term="深度学习基础" scheme="https://luoyongjia.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
  </entry>
  
  <entry>
    <title>hexo Latex显示问题</title>
    <link href="https://luoyongjia.github.io/2021/04/07/hexo-Latex%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98/"/>
    <id>https://luoyongjia.github.io/2021/04/07/hexo-Latex%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98/</id>
    <published>2021-04-07T07:21:08.000Z</published>
    <updated>2021-04-07T07:31:36.392Z</updated>
    
    <content type="html"><![CDATA[<p>近几天突然发现博客上<script type="math/tex">\latex</script>公式不显示了，前段时间还好好的，并且在本地的<code>locallhost:4000</code>上很正常。来一下解决方案，或许以后还能用到。</p><p>我的博客用的是<code>pure</code>主题。它的配置文件中并没有提到mathjax，所以我是手动加的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mathjax: true</span><br></pre></td></tr></table></figure><p>也用了一段时间。</p><h1 id="尝试"><a href="#尝试" class="headerlink" title="尝试"></a>尝试</h1><ul><li>安装了插件、false</li><li>换了个浏览器、false</li><li>肯定不是网的问题，网一直都在墙外呆着、false</li></ul><h1 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h1><p>看到<code>NexT</code>主题中关于mathjax的配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># MathJax Support</span><br><span class="line">mathjax:</span><br><span class="line">  enable: false</span><br><span class="line">  per_page: false</span><br><span class="line">  cdn: &#x2F;&#x2F;cdn.mathjax.org&#x2F;mathjax&#x2F;latest&#x2F;MathJax.js?config&#x3D;TeX-AMS-MML_HTMLorMML</span><br></pre></td></tr></table></figure><p>把这段整到我的主题的配置文件里面去，就ok惹。🎉🎉</p><h6 id="refer"><a href="#refer" class="headerlink" title="refer"></a>refer</h6><hr><p><a href="https://www.lizhechen.com/2018/10/04/%E8%AE%B0%E4%B8%80%E6%AC%A1%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%94%B9%E7%89%88/">调教Hexo[4]——记一次费劲的改版</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;近几天突然发现博客上&lt;script type=&quot;math/tex&quot;&gt;\latex&lt;/script&gt;公式不显示了，前段时间还好好的，并且在本地的&lt;code&gt;locallhost:4000&lt;/code&gt;上很正常。来一下解决方案，或许以后还能用到。&lt;/p&gt;
&lt;p&gt;我的博客用的是&lt;</summary>
      
    
    
    
    <category term="基本操作" scheme="https://luoyongjia.github.io/categories/%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    
    
    <category term="hexo" scheme="https://luoyongjia.github.io/tags/hexo/"/>
    
    <category term="latex" scheme="https://luoyongjia.github.io/tags/latex/"/>
    
  </entry>
  
  <entry>
    <title>【ML】Regression</title>
    <link href="https://luoyongjia.github.io/2021/04/01/%E3%80%90ML%E3%80%91Regression/"/>
    <id>https://luoyongjia.github.io/2021/04/01/%E3%80%90ML%E3%80%91Regression/</id>
    <published>2021-04-01T13:06:44.000Z</published>
    <updated>2021-04-07T07:16:44.293Z</updated>
    
    <content type="html"><![CDATA[<p>这是李宏毅深度学习2020的第一课笔记。</p><h1 id="Regression在做什么"><a href="#Regression在做什么" class="headerlink" title="Regression在做什么"></a>Regression在做什么</h1><p>输入数据，输出一个<strong>数值</strong>。    </p><ul><li>股票预测<ul><li><script type="math/tex; mode=display">f(StocksInfo) = DowJonesIndustrialAverage atTomorrow</script></li></ul></li><li>自动驾驶<ul><li><script type="math/tex">f(Road Info) =</script> 方向盘的角度</li></ul></li></ul><h1 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h1><p>课上用宝可梦的例子来解释了这个步骤。通过一个基础宝可梦的相关数据预测其进化后的Combat Power(CP)</p><h2 id="Step-1-Model"><a href="#Step-1-Model" class="headerlink" title="Step 1: Model"></a>Step 1: Model</h2><p>从一堆不同的函数中（一次函数、二次函数、对数函数等），选取合适当前数据的函数。</p><p>这里选取了线性模型：<script type="math/tex">y = b + \sum w_{i}x_{i}</script><br>feature: <script type="math/tex">x_{i}: x_{cp}, x_{hp}, x_{w}, x_{h}...</script></p><p><img src="/2021/04/01/%E3%80%90ML%E3%80%91Regression/1.png" alt="f1"></p><p>挑了<script type="math/tex">x_{cp}</script>作为天选之子，不同的<script type="math/tex">w</script>和<script type="math/tex">b</script>的选择能构成各种各样的函数：    </p><script type="math/tex; mode=display">\begin{equation*}\begin{aligned}&f1: y = 10.0 + 9.0\cdot x_{cp}\\&f2: y = 9.8 + 9.2\cdot x_{cp}\\&f3: y = -0.8 - 1.2\cdot x_{cp}\\\end{aligned}\end{equation*}</script><h2 id="Step-2-Goodness-of-Function"><a href="#Step-2-Goodness-of-Function" class="headerlink" title="Step 2: Goodness of Function"></a>Step 2: Goodness of Function</h2><p>现在有那么多的函式，想要挑出能最好预测宝可梦cp值的函式需要有一个<strong>评判标准</strong>。这时候，Loss就出来了。</p><h3 id="先来观察下Training-Data"><a href="#先来观察下Training-Data" class="headerlink" title="先来观察下Training Data"></a>先来观察下Training Data</h3><p>Train Data由函数输入<script type="math/tex">x^{i}</script>和函数正确的输出<script type="math/tex">\widehat{y}^{i}</script>。<br><img src="/2021/04/01/%E3%80%90ML%E3%80%91Regression/2.png" alt="f2"><br>课上老师的数据中有十组宝可梦的数据<script type="math/tex">(x_{cp}^{n}, \widehat{y}^{n})</script></p><h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><p>Input: a function<br>Output: how bat it is        </p><p>这里定义loss为：        </p><script type="math/tex; mode=display">L(f) = \sum_{n = 1}^{10}(\widehat{y}^{n} - f(x_{cp}^{n}))^{2}</script><p>也就是：    </p><script type="math/tex; mode=display">L(w,b) = \sum_{n = 1}^{10}(\widehat{y}^{n} - (b + w \cdot x_{cp}^{n}))^{2}</script><p>Loss function的值越小，表示函数预测cp值的效果越好。</p><h2 id="Step-3-Best-Function"><a href="#Step-3-Best-Function" class="headerlink" title="Step 3: Best Function"></a>Step 3: Best Function</h2><p>诶嘿，终于要找最好的预测函数了。在上一步中，我们定义了Loss Function，这里需要做的工作就是将Loss Function的值降到最低。课上使用了梯度下降。</p><p>工作：    </p><script type="math/tex; mode=display">\begin{equation*}\begin{aligned}f^{*} &= arg \mathop{min}\limits_{f}L(f)\\w^{*}, b^{*} &= arg \mathop{min}\limits_{w,b} L(w,b)\\& = arg \mathop{min}\limits_{w,b} \sum_{n = 1}^{10}(\widehat{y}^{n} - (b + w \cdot x_{cp}^{n}))^{2}\\end{aligned}\end{equation*}</script><h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p>梯度下降，最简单的梯度下降。</p><ol><li>随机初始化<script type="math/tex">w^{0}</script></li><li>计算梯度<script type="math/tex">\frac{dL}{dw}|_{w = w^{0}}</script><script type="math/tex; mode=display">\begin{equation*}\begin{aligned}&w^{1} = w^{0} - \eta \frac{dL}{dw}|_{w = w^{0}}\\&w^{2} = w^{1} - \eta \frac{dL}{dw}|_{w = w^{1}}\\& \ldots\end{aligned}\end{equation*}</script></li></ol><p><img src="/2021/04/01/%E3%80%90ML%E3%80%91Regression/3.png" alt="f3"><br>梯度为负时，w++，梯度为正时，w—就这样一步步重复计算梯度-&gt;移动w的过程。</p><h4 id="Gradient是什么"><a href="#Gradient是什么" class="headerlink" title="Gradient是什么"></a>Gradient是什么</h4><p>就是对于多个参数组成的向量。<br><img src="/2021/04/01/%E3%80%90ML%E3%80%91Regression/4.png" alt="f4">    </p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><h3 id="Average-Error"><a href="#Average-Error" class="headerlink" title="Average Error"></a>Average Error</h3><script type="math/tex; mode=display">\frac{1}{n}\sum_{i = 1}^{n}e^{i}</script><p><script type="math/tex">\sum</script>|真实值-预测值|</p><h3 id="探索不同的model的效果"><a href="#探索不同的model的效果" class="headerlink" title="探索不同的model的效果"></a>探索不同的model的效果</h3><p>这里就不一一举例了，直接来个结果：<br>次数越高的model，对于train data的拟合效果越好，能表示的数据结构更多。但是这样会使得再test data上效果并不好，这就是<strong>过拟合</strong>。所以要找一个合适的model，能够做到train data和test data两个性能的兼顾。</p><p><img src="/2021/04/01/%E3%80%90ML%E3%80%91Regression/5.png" alt="f5"></p><h3 id="多feature"><a href="#多feature" class="headerlink" title="多feature"></a>多feature</h3><p>观察更多的数据，发现并不只当前cp值影响预测结果，还有很多其他的因素。这时候<script type="math/tex">x</script>就不是一个单纯的数值，而是一个能够表现多个特征的向量。暂时，是这么理解的。</p><h6 id="refer"><a href="#refer" class="headerlink" title="refer"></a>refer</h6><hr><p><a href="https://www.youtube.com/watch?v=fegAeph9UaA">ML讲座1：回归 - 案例研究</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这是李宏毅深度学习2020的第一课笔记。&lt;/p&gt;
&lt;h1 id=&quot;Regression在做什么&quot;&gt;&lt;a href=&quot;#Regression在做什么&quot; class=&quot;headerlink&quot; title=&quot;Regression在做什么&quot;&gt;&lt;/a&gt;Regression在做什么&lt;/</summary>
      
    
    
    
    <category term="深度学习基础" scheme="https://luoyongjia.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
  </entry>
  
  <entry>
    <title>conda常用操作</title>
    <link href="https://luoyongjia.github.io/2021/03/30/conda%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/"/>
    <id>https://luoyongjia.github.io/2021/03/30/conda%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</id>
    <published>2021-03-30T11:35:27.000Z</published>
    <updated>2021-03-30T12:15:25.499Z</updated>
    
    <content type="html"><![CDATA[<h3 id="升级conda"><a href="#升级conda" class="headerlink" title="升级conda"></a>升级conda</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda update conda</span><br></pre></td></tr></table></figure><h3 id="升级anaconda"><a href="#升级anaconda" class="headerlink" title="升级anaconda"></a>升级anaconda</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda update anaconda</span><br></pre></td></tr></table></figure><h1 id="基本"><a href="#基本" class="headerlink" title="基本"></a>基本</h1><h3 id="显示config"><a href="#显示config" class="headerlink" title="显示config"></a>显示config</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda config --show</span><br></pre></td></tr></table></figure><h3 id="查看channels"><a href="#查看channels" class="headerlink" title="查看channels"></a>查看channels</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda config --show channels</span><br></pre></td></tr></table></figure><h3 id="移除镜像源"><a href="#移除镜像源" class="headerlink" title="移除镜像源"></a>移除镜像源</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda config --remove channels (url)</span><br></pre></td></tr></table></figure><h3 id="添加镜像源"><a href="#添加镜像源" class="headerlink" title="添加镜像源"></a>添加镜像源</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels (url)</span><br></pre></td></tr></table></figure><h1 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h1><h3 id="创建环境"><a href="#创建环境" class="headerlink" title="创建环境"></a>创建环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n Name python&#x3D;x.x</span><br></pre></td></tr></table></figure><h3 id="进入环境"><a href="#进入环境" class="headerlink" title="进入环境"></a>进入环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source activate Name</span><br></pre></td></tr></table></figure><h3 id="查看某环境下的包"><a href="#查看某环境下的包" class="headerlink" title="查看某环境下的包"></a>查看某环境下的包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda list -n Name</span><br></pre></td></tr></table></figure><h3 id="给某环境安装包"><a href="#给某环境安装包" class="headerlink" title="给某环境安装包"></a>给某环境安装包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -n Name package</span><br></pre></td></tr></table></figure><h3 id="退出环境"><a href="#退出环境" class="headerlink" title="退出环境"></a>退出环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source deactivate</span><br></pre></td></tr></table></figure><h1 id="包"><a href="#包" class="headerlink" title="包"></a>包</h1><h3 id="搜索指定的包"><a href="#搜索指定的包" class="headerlink" title="搜索指定的包"></a>搜索指定的包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda search package</span><br></pre></td></tr></table></figure><h3 id="更新当前环境所有包"><a href="#更新当前环境所有包" class="headerlink" title="更新当前环境所有包"></a>更新当前环境所有包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda update --all</span><br></pre></td></tr></table></figure><h3 id="安装包"><a href="#安装包" class="headerlink" title="安装包"></a>安装包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install package</span><br></pre></td></tr></table></figure><h3 id="更新包"><a href="#更新包" class="headerlink" title="更新包"></a>更新包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda update package</span><br></pre></td></tr></table></figure><h6 id="refer"><a href="#refer" class="headerlink" title="refer"></a>refer</h6><hr><p><a href="https://blog.csdn.net/qq_37405118/article/details/106003069">conda 基本操作（常用的）</a><br><a href="https://zhuanlan.zhihu.com/p/103134000">利用conda升级Anaconda及其包</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;升级conda&quot;&gt;&lt;a href=&quot;#升级conda&quot; class=&quot;headerlink&quot; title=&quot;升级conda&quot;&gt;&lt;/a&gt;升级conda&lt;/h3&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=</summary>
      
    
    
    
    <category term="基本操作" scheme="https://luoyongjia.github.io/categories/%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux查看GPU状态</title>
    <link href="https://luoyongjia.github.io/2021/03/30/Linux%E6%9F%A5%E7%9C%8BGPU%E7%8A%B6%E6%80%81/"/>
    <id>https://luoyongjia.github.io/2021/03/30/Linux%E6%9F%A5%E7%9C%8BGPU%E7%8A%B6%E6%80%81/</id>
    <published>2021-03-30T02:41:08.000Z</published>
    <updated>2021-03-30T11:32:42.549Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本"><a href="#基本" class="headerlink" title="基本"></a>基本</h2><p><strong>查看显卡信息</strong>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lspci | grep -i vga</span><br></pre></td></tr></table></figure></p><p>然后会显示一条条的显卡信息。然后用<code>lspci -v -s 00:0X:XX</code>，查看相应的显卡信息。<br><strong>nvida GPU查看</strong>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lspci | grep -i nvidia</span><br></pre></td></tr></table></figure></p><h2 id="Nvidia表格"><a href="#Nvidia表格" class="headerlink" title="Nvidia表格"></a>Nvidia表格</h2><p><strong>Nvidia显卡信息及表头解释</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure><p><img src="/2021/03/30/Linux%E6%9F%A5%E7%9C%8BGPU%E7%8A%B6%E6%80%81/1.png" alt="f1"></p><h3 id="上层"><a href="#上层" class="headerlink" title="上层"></a>上层</h3><ul><li>Fan: 风扇转速（0-100%），计算机的期望转速。N/A表示计算机不是风冷或风扇坏了。</li><li>Temp: 显卡内部温度（摄氏度）。</li><li>Perf: 性能状态，P0-P12，P0表示最大性能。</li><li>Pwr: 能耗。</li><li>Bus-Id: GPU总线相关信息。</li><li>Disp.A: Display Active, GPU是否初始化。</li><li>Memory Usage: 显存使用率。</li><li>Volatile GPU-Util: 浮动的GPU使用率。</li><li>Compute M: 计算模式。</li></ul><h3 id="Processes"><a href="#Processes" class="headerlink" title="Processes"></a>Processes</h3><p>显示每块GPU上每个进程所使用的现存情况。</p><h6 id="refer"><a href="#refer" class="headerlink" title="refer"></a>refer</h6><hr><p><a href="https://blog.csdn.net/dcrmg/article/details/78146797">Linux查看GPU信息和使用情况</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本&quot;&gt;&lt;a href=&quot;#基本&quot; class=&quot;headerlink&quot; title=&quot;基本&quot;&gt;&lt;/a&gt;基本&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;查看显卡信息&lt;/strong&gt;：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr</summary>
      
    
    
    
    <category term="基本操作" scheme="https://luoyongjia.github.io/categories/%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    
    
    <category term="GPU" scheme="https://luoyongjia.github.io/tags/GPU/"/>
    
    <category term="Linux" scheme="https://luoyongjia.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>神经网络结构搜索（NAS）综述0.1</title>
    <link href="https://luoyongjia.github.io/2021/03/29/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E6%90%9C%E7%B4%A2%EF%BC%88NAS%EF%BC%89%E7%BB%BC%E8%BF%B00-1/"/>
    <id>https://luoyongjia.github.io/2021/03/29/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E6%90%9C%E7%B4%A2%EF%BC%88NAS%EF%BC%89%E7%BB%BC%E8%BF%B00-1/</id>
    <published>2021-03-29T02:37:04.000Z</published>
    <updated>2021-03-30T02:35:23.867Z</updated>
    
    <content type="html"><![CDATA[<p>近期读了一篇Nas综述（<a href="https://arxiv.org/abs/1808.05377">Neural Architecture Search: A Survey</a>）了，这篇是一篇19年的综述…就这两年Nas的发展可以说是老综述了；但是更新的我也没找到，所以就凑活着看看。</p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>首先是<strong>深度学习</strong>在各个领域的应用。随着深度学习的发展，网络结构的设计对于完成任务至关重要。但是很多网络结构的设计都是基于人的经验的，这很容易出错。所以将构建网络这个过程，也交给数据与机器去自动选择。</p><h1 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h1><p>能够快速地找到适应一个任务的网络结构。<br>如图所示：</p><p><img src="/2021/03/29/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E6%90%9C%E7%B4%A2%EF%BC%88NAS%EF%BC%89%E7%BB%BC%E8%BF%B00-1/1.jpg" alt="f1"></p><h1 id="经典方法"><a href="#经典方法" class="headerlink" title="经典方法"></a>经典方法</h1><p><img src="/2021/03/29/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E6%90%9C%E7%B4%A2%EF%BC%88NAS%EF%BC%89%E7%BB%BC%E8%BF%B00-1/2.png" alt="f2"></p><p>主要包含三个方向：<strong>搜索空间</strong>、<strong>搜索策略</strong>、<strong>评价预估</strong>。很好理解：<br><strong>搜索空间</strong> 从哪些网络结构中搜索<br><strong>搜索策略</strong> 怎么针对任务挑出适应的结构<br><strong>评价预估</strong> 具像化简单的理解，就是评价搜索策略找出来的这个网络结构的性能。        </p><h1 id="Search-Space"><a href="#Search-Space" class="headerlink" title="Search Space"></a>Search Space</h1><p><script type="math/tex">chain-structured neural networks</script>: 链式的网络结构，搜索的可以有：1. 网络的层数；2. 每一层的操作；3. 超参。<br>如图中左边的结构。</p><p><script type="math/tex">multi-branch networks</script>: 就字面意思，多分支，看图中右边结构就懂了。</p><p><img src="/2021/03/29/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E6%90%9C%E7%B4%A2%EF%BC%88NAS%EF%BC%89%E7%BB%BC%E8%BF%B00-1/3.png" alt="f3"></p><p><script type="math/tex">cells</script>：人工设计的复杂的网络中有很多重复的部分，大约就是，训练小的cell，然后再进行堆叠。</p><p><img src="/2021/03/29/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E6%90%9C%E7%B4%A2%EF%BC%88NAS%EF%BC%89%E7%BB%BC%E8%BF%B00-1/4.png" alt="f4"></p><h1 id="Search-Strategy"><a href="#Search-Strategy" class="headerlink" title="Search Strategy"></a>Search Strategy</h1><h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><p>这个暂时还不懂。</p><h2 id="遗传进化算法"><a href="#遗传进化算法" class="headerlink" title="遗传进化算法"></a>遗传进化算法</h2><p>大约的框架就是：</p><blockquote><ol><li>随机生成一个种群（n个网络结构）</li><li>循环：选择、交叉、变异，直到满足最终条件</li></ol></blockquote><p>不同的工作可以聚焦在不同的过程中，比如如何sample种群，如何选择种群等。</p><p>我毕设中就是用的这种方法。</p><h2 id="基于梯度的方法"><a href="#基于梯度的方法" class="headerlink" title="基于梯度的方法"></a>基于梯度的方法</h2><p>强化学习和遗传进化算法都是在离散的空间中搜索，这种方法就是把操作与操作之间的路径选择转基于选择某路径的概率，从而使得离散的问题变成了连续的问题，就可以基于梯度优化的方法进行网络结构搜索了。</p><h1 id="Performance-Estimation-Strategy"><a href="#Performance-Estimation-Strategy" class="headerlink" title="Performance Estimation Strategy"></a>Performance Estimation Strategy</h1><p>诶嘿，这块儿是一个加快NAS训练的重要点。然后我挑了俩个人觉得还算靠谱的方法。</p><h2 id="Weight-Inheritance"><a href="#Weight-Inheritance" class="headerlink" title="Weight Inheritance"></a>Weight Inheritance</h2><p>参数级别的迁移，用之前已经训练好的模型权重参数对于目标问题赋值，从一个较高起点的初值开始搜。具体怎么搞的，以后再补充。</p><h2 id="One-shot"><a href="#One-shot" class="headerlink" title="One-shot"></a>One-shot</h2><p>在其他的方法中都需要对于每一个（<script type="math/tex">n</script>）网络结构进行训练。所以One-shot把这<script type="math/tex">n</script>个网络结构使用一个SuperNet表示。SuperNet中，可以表示不同的SubNet，从而覆盖了之前的<script type="math/tex">n</script>个网络结构。通过对于这个SuperNet的训练达到了训练<script type="math/tex">n</script>个网络结构的效果。从而大大减少了训练所需的时间和资源。</p><h1 id="自己的碎碎念"><a href="#自己的碎碎念" class="headerlink" title="自己的碎碎念"></a>自己的碎碎念</h1><p>之前了解得不多，怎么说呢，想要开始就整这个方向。现在看了🤏相关的知识，会发现这其实只是一种方法，也可以说是一种工具吧，并不是一个好的，作为research起点的方向。确实这个方向有很多值得研究的方面，但是我觉得都是需要基于个人对于另一个什么方向有了深刻的理解了，再来，或许是优化NAS的方法，又或许是NAS与某个方面结合，亦或许是使NAS能做到多任务。这都是没有好的其他的基础，做不到的，so.</p><h6 id="refer"><a href="#refer" class="headerlink" title="refer"></a>refer</h6><hr><p><a href="https://blog.csdn.net/qq_41997920/article/details/92760903">神经网络架构搜索（NAS）综述</a><br><a href="https://arxiv.org/abs/1808.05377">Neural Architecture Search: A Survey</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;近期读了一篇Nas综述（&lt;a href=&quot;https://arxiv.org/abs/1808.05377&quot;&gt;Neural Architecture Search: A Survey&lt;/a&gt;）了，这篇是一篇19年的综述…就这两年Nas的发展可以说是老综述了；但是更新的我也没</summary>
      
    
    
    
    <category term="Research" scheme="https://luoyongjia.github.io/categories/Research/"/>
    
    
    <category term="Nas" scheme="https://luoyongjia.github.io/tags/Nas/"/>
    
  </entry>
  
  <entry>
    <title>Mac配置Anaconda环境变量</title>
    <link href="https://luoyongjia.github.io/2021/03/26/Mac%E9%85%8D%E7%BD%AEAnaconda%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/"/>
    <id>https://luoyongjia.github.io/2021/03/26/Mac%E9%85%8D%E7%BD%AEAnaconda%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/</id>
    <published>2021-03-26T05:15:34.000Z</published>
    <updated>2021-03-26T05:23:46.212Z</updated>
    
    <content type="html"><![CDATA[<p>因为好像就是这两年，macOS的系统大更新，把默认shell的bash改成了zsh。导致以前的一些配置环境变量的东西都没法儿用了。今天要开始看python的代码了，所以想着来把anaconda配一下。真的太久没用了，导致版本都……所以需要在终端配置一下，居然发现我的终端的conda, not found了。就来解决了一下。</p><h2 id="机器背景"><a href="#机器背景" class="headerlink" title="机器背景"></a>机器背景</h2><p>mbp2018, bash。</p><p>会发现，原来的<code>bash_profile</code>都不管用了<br>而且，anaconda的默认路径也更改了。</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>anaconda的路径: <code>/Users/USERNAME/opt/anaconda3/bin</code></p><p>bash配置文件路径：<code>/private/etc/bashrc</code></p><p>就，强制在bashrc文件里面加<code>export PATH=&quot;/Users/USERNAME/opt/anaconda3/bin:$PATH&quot;</code><br>即可。</p><p>暂时还没出现问题，后续再说吧。</p><h6 id="refer"><a href="#refer" class="headerlink" title="refer"></a>refer</h6><hr><p><a href="https://zhuanlan.zhihu.com/p/121086727">Mac 没有找到 conda命令</a><br><a href="https://discussionschinese.apple.com/thread/251633370">mac设置全局环境变量</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;因为好像就是这两年，macOS的系统大更新，把默认shell的bash改成了zsh。导致以前的一些配置环境变量的东西都没法儿用了。今天要开始看python的代码了，所以想着来把anaconda配一下。真的太久没用了，导致版本都……所以需要在终端配置一下，居然发现我的终端的c</summary>
      
    
    
    
    
    <category term="环境变量" scheme="https://luoyongjia.github.io/tags/%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>【数据结构】串</title>
    <link href="https://luoyongjia.github.io/2021/03/18/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E4%B8%B2/"/>
    <id>https://luoyongjia.github.io/2021/03/18/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E4%B8%B2/</id>
    <published>2021-03-18T02:08:37.000Z</published>
    <updated>2021-03-18T04:01:39.258Z</updated>
    
    <content type="html"><![CDATA[<h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><p>计算机上非数值的处理对象。</p><h1 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h1><p><strong>Copy(&amp;T, s)</strong> 就是copy操作，将一个地址里的东西取出来，存到新的字符串里面。<br><strong>length()</strong> 字符串的长度。<br><strong>Index(S, T)</strong> 定位t的初始位置。</p><h1 id="存储结构"><a href="#存储结构" class="headerlink" title="存储结构"></a>存储结构</h1><h2 id="定长顺序存储"><a href="#定长顺序存储" class="headerlink" title="定长顺序存储"></a>定长顺序存储</h2><p>顺序存储结构，分配一个定长的空间存储字符串。</p><h2 id="堆分配存储"><a href="#堆分配存储" class="headerlink" title="堆分配存储"></a>堆分配存储</h2><p>是在程序运行的过程中，动态分配形成的。当程序请求一块空间之后，在堆空间中分配一块区域。不成功返回null。</p><h2 id="块链存储"><a href="#块链存储" class="headerlink" title="块链存储"></a>块链存储</h2><p>类似于链式存储。多个节点，每个节点里面放几个字符。空的用“#”补全。</p><h1 id="字符串算法"><a href="#字符串算法" class="headerlink" title="字符串算法"></a>字符串算法</h1><h2 id="暴力匹配"><a href="#暴力匹配" class="headerlink" title="暴力匹配"></a>暴力匹配</h2><p>老暴力方法了，就两个字符串硬比，没什么技术水平。</p><h2 id="KMP"><a href="#KMP" class="headerlink" title="KMP"></a>KMP</h2><p>这才是重中之重！<br>为什么有这种方法呢，因为在暴力算法中重复了大量的比较，这种方法能够减少这种重复的出现。</p><h3 id="先来一个文字描述"><a href="#先来一个文字描述" class="headerlink" title="先来一个文字描述"></a>先来一个文字描述</h3><blockquote><p><strong>什么是next数组</strong><br>next数组就是匹配字串的每一位所在的最长前后缀相同的长度。<br><br><strong>如何构建呢？<br></strong></p><ol><li>我使用的是动态规划的方法。前后两个指针。前指针指向后缀的最后一个字符，后指针指向后缀的最后一个字符。比较这两个指针存的内容，<br></li><li>当前指针为0，且前指针与后指针内容不相等的时候，后指针所指下标next数组存为0<br></li><li>相同则后指针所在位置等于前指针前一位next数组存的数字+1<br></li><li>不同则将前指针移动到前指针前一位next数组存的数字地址再做比较，直到前指针到0。<br><br><strong>如何匹配<br></strong></li><li>正常匹配<br></li><li>当遇到两个字符不相等的时候，将母串与子串的next数组对应的前一位的内容做对比，相当于将子串移动到前一next位。这样就减少了前一next之前的那些重复的比较。</li></ol></blockquote><h3 id="再来一个代码"><a href="#再来一个代码" class="headerlink" title="再来一个代码"></a>再来一个代码</h3><p>代码再说吧，有时间再写。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h1&gt;&lt;p&gt;计算机上非数值的处理对象。&lt;/p&gt;
&lt;h1 id=&quot;基本操作&quot;&gt;&lt;a href=&quot;#基本操作&quot; class=&quot;headerlink&quot; tit</summary>
      
    
    
    
    <category term="数据结构" scheme="https://luoyongjia.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>【TOEFL】 口语Task1、Task2</title>
    <link href="https://luoyongjia.github.io/2021/03/11/%E3%80%90TOEFL%E3%80%91-%E5%8F%A3%E8%AF%ADTask1%E3%80%81Task2/"/>
    <id>https://luoyongjia.github.io/2021/03/11/%E3%80%90TOEFL%E3%80%91-%E5%8F%A3%E8%AF%ADTask1%E3%80%81Task2/</id>
    <published>2021-03-11T02:28:33.000Z</published>
    <updated>2021-03-11T02:54:07.758Z</updated>
    
    <content type="html"><![CDATA[<h1 id="review"><a href="#review" class="headerlink" title="review"></a>review</h1><h2 id="口语的评分标准"><a href="#口语的评分标准" class="headerlink" title="口语的评分标准"></a>口语的评分标准</h2><ol><li>dilivery: 表达，语言表达是否准确</li><li>language use: 用词和语法是否恰当</li><li>topic development: <ol><li>独立：每个点都要有一定的展开</li><li>综合：文中或者是听力中的每个细节都要提到 </li></ol></li></ol><h1 id="task1"><a href="#task1" class="headerlink" title="task1"></a>task1</h1><p>主要是如何展开，想论据这样子。</p><h2 id="名词"><a href="#名词" class="headerlink" title="名词"></a>名词</h2><h3 id="角色"><a href="#角色" class="headerlink" title="角色"></a>角色</h3><h4 id="立场"><a href="#立场" class="headerlink" title="立场"></a>立场</h4><p>不同的角色有不同的立场，从不同的立场去考虑这个问题。比如说，junk food AD，就有parents和children两个方面去考虑这个广告的影响。</p><h4 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h4><p>不同的角色有不同的特点。还是那个junk food的例子，可能children就有容易被影响的特点。</p><h3 id="其他名词"><a href="#其他名词" class="headerlink" title="其他名词"></a>其他名词</h3><p>分析有没有具体的细节、具体的功能、具体的表现。比如那题，现代科技代替图书馆的题，就可以分析现代科技的功能，分析图书馆的功能，从这些角度来答题。</p><h1 id="task2"><a href="#task2" class="headerlink" title="task2"></a>task2</h1><h2 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h2><p>校方通知，学生建议</p><h2 id="阅读中的注意点"><a href="#阅读中的注意点" class="headerlink" title="阅读中的注意点"></a>阅读中的注意点</h2><ul><li>改变：正文标题，第一句话。</li><li>数字：如果文中有数字，一定要记下来，在答题中打出来显得十分细节</li><li>姓名：写信的学生的姓名，然后就是教授的名字</li><li>借口：一般会有两个接口，记个大概，因为有时候会让重复、总结文中的理由。学生的建议有时候会需要答。</li></ul><h2 id="答题模版"><a href="#答题模版" class="headerlink" title="答题模版"></a>答题模版</h2><ol><li>The university gives an announcement that/of do/doing …<br>Student xxx suggests/proposes the university to do …</li><li>agrees: feels happy with, infovor of <br>disagrees: feels sorry for</li><li>attrcting, enhence: 文中的理由和听力中的相同，就增强。不同就另说。</li></ol><h2 id="增长时间利器"><a href="#增长时间利器" class="headerlink" title="增长时间利器"></a>增长时间利器</h2><p>如果答完自己记录的细节还有5s多，那就来一句废话，就是关于第2个理由的废话，时间就够啦。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;review&quot;&gt;&lt;a href=&quot;#review&quot; class=&quot;headerlink&quot; title=&quot;review&quot;&gt;&lt;/a&gt;review&lt;/h1&gt;&lt;h2 id=&quot;口语的评分标准&quot;&gt;&lt;a href=&quot;#口语的评分标准&quot; class=&quot;headerlink&quot; ti</summary>
      
    
    
    
    <category term="TOEFL" scheme="https://luoyongjia.github.io/categories/TOEFL/"/>
    
    
    <category term="口语" scheme="https://luoyongjia.github.io/tags/%E5%8F%A3%E8%AF%AD/"/>
    
  </entry>
  
  <entry>
    <title>【数据结构】栈和队列</title>
    <link href="https://luoyongjia.github.io/2021/03/10/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97/"/>
    <id>https://luoyongjia.github.io/2021/03/10/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97/</id>
    <published>2021-03-10T12:51:00.000Z</published>
    <updated>2021-03-11T02:26:17.195Z</updated>
    
    <content type="html"><![CDATA[<h1 id="栈"><a href="#栈" class="headerlink" title="栈"></a>栈</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>是线性表的应用和推广。是受限的线性表。栈，就是“先进后出”。就像一个只有一个出口的桶，先进去的人只能等后进去的人走了才能出去。</p><h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><p><strong>isEmpty()</strong> 栈是否为空<br><strong>push(&amp;s, T elem)</strong> 压入栈<br><strong>pop(&amp;s, T elem)</strong> 吐出最顶上那个元素<br><strong>getTop(s, &amp;x)</strong>  把最顶上那个复制一份给x<br><strong>destroyStack()</strong> 毁灭吧，栈</p><h2 id="存储结构"><a href="#存储结构" class="headerlink" title="存储结构"></a>存储结构</h2><h3 id="顺序存储—-gt-顺序栈"><a href="#顺序存储—-gt-顺序栈" class="headerlink" title="顺序存储—&gt;顺序栈"></a>顺序存储—&gt;顺序栈</h3><p>就是用数组来存，然后有一个位置指针来操作。</p><h4 id="共享栈"><a href="#共享栈" class="headerlink" title="共享栈"></a>共享栈</h4><p>顺序栈的抠门做法。一个空间俩栈用。一个从头开始，一个从尾巴开始这样子。</p><h3 id="链式存储"><a href="#链式存储" class="headerlink" title="链式存储"></a>链式存储</h3><p>就叫链栈。便于多个栈共享存储空间，提高效率。不存在放不下的问题，反正链式，可以无限扩展。</p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>算式转换，中序转后序。这个不好怎么解释，就来个🌰体会一下。</p><blockquote><p>中序表达式 <code>a + b - a * ((c + d) / e - f) + g</code>转换为等价的后缀表达式为<code>ab+acd+e/f-*-g+</code><br><br>过程大约是这个样子的：<br></p><ol><li>最先压入一个“#”<br></li><li>然后遇到数字直接输出<br></li><li>遇到符号根据优先级来，如果后一个的优先级高于栈顶的，则直接压入；如果后一个的优先级低于栈顶的，则弹出栈顶的，再做比较。</li></ol></blockquote><p>来一道例题吧</p><blockquote><p>假设栈初始为空，将中序表达式<code>a / b + (c * d - e * f) / g</code>转换为等价的后缀表达式的过程中，当扫描到<code>f</code>时，栈中的元素依次是<br><br>A.<code>+(*-</code> B. <code>+(-*</code> C. <code>/+(*-*</code> D. <code>/+-*</code><br><br><br><br>直接来个答案吧，B</p></blockquote><h2 id="题"><a href="#题" class="headerlink" title="题"></a>题</h2><blockquote><ol><li>设链表不带头节点，且所有操作均在表头进行，则下列最不适合作为链栈的是（ ）<br><br> A. 只有表头节点指针，没有表尾指针的双向循环链表<br> B. 只有表尾节点指针，没有表头指针的双向循环链表<br>C. 只有表头节点指针，没有表尾指针的单项循环链表<br>D.只有表尾节点指针，没有表头指针的单项循环链表<br><br><br><br>C. 因为这里只有表头，没有表尾。每次对于栈的操作都是在头上做的。但是为了维持循环链表，所以得找到尾，所以所有操作至少都是<script type="math/tex">O(n)</script>，所以它不合适。<br><br></li><li><p>若已知一个栈的入栈序列是1，2，3，4，其出栈序列为<script type="math/tex">P_{1}, P_{2}, P_{3}, P_{4}</script>, 则<script type="math/tex">P_{2}, P_{4}</script>不可能是（ ）<br><br>A. 2，4  B. 2，1 C. 4，3 D.<br><br><br><br>这种题目，就只有一种方法，那就是自己一个个举例子。<br><br></p></li><li><p>设栈的初始状态为空，当字符序列“n1_”作为栈的输入时，输出长度为3，且可用作C语言标识符的序列有（ ）<br><br>A. 4 B. 5 C. 3 D. 6<br><br><br><br>这个题我个人觉得出的还挺好的，结合了数据结构和C语言的知识。这里，C语言标识符的要求是“第1个字符只能是字母或是下划线。”</p></li></ol></blockquote><h1 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h1><h2 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h2><p>也是特殊的线性表。先进先出，就跟那吸管一样，只有一头可以进，另一头可以出。</p><h2 id="基本操作-1"><a href="#基本操作-1" class="headerlink" title="基本操作"></a>基本操作</h2><p><strong>front</strong> 头<br><strong>rear</strong> 尾<br><strong>enQueue()</strong> 压入<br><strong>deQueue()</strong> 出队列<br><strong>getHead()</strong> 获取头</p><h2 id="存储结构-1"><a href="#存储结构-1" class="headerlink" title="存储结构"></a>存储结构</h2><h3 id="顺序存储"><a href="#顺序存储" class="headerlink" title="顺序存储"></a>顺序存储</h3><p>“假溢出”。要是用传统的一条的那种数组来存队列的话，进进出出几次，下标就给整没了，但是其实那时候队列里面还是有空间存储元素的。</p><h4 id="循环队列"><a href="#循环队列" class="headerlink" title="循环队列"></a>循环队列</h4><p>所以人们就变聪明了，用环形的来存储队列。这就涉及一个下标计算的问题，这个问题常考，所以要记得一些公式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">队首指针进1: front &#x3D; (front + 1) % MaxSize</span><br><span class="line">队尾指针进1:rear &#x3D; (rear + 1) %MaxSize</span><br><span class="line">队列长度:(rear + MaxSize - front) %MaxSize</span><br><span class="line">队满：(rear + 1) % MaxSize &#x3D;&#x3D; front</span><br><span class="line">空：front &#x3D;&#x3D; rear</span><br></pre></td></tr></table></figure><h3 id="链式存储-1"><a href="#链式存储-1" class="headerlink" title="链式存储"></a>链式存储</h3><p>带头节点单链表。有一个头头节点，然后有一个rear节点，这样子就能够很好的表示队列。</p><h2 id="奇奇怪怪的队列增加了"><a href="#奇奇怪怪的队列增加了" class="headerlink" title="奇奇怪怪的队列增加了"></a>奇奇怪怪的队列增加了</h2><h3 id="双端队列"><a href="#双端队列" class="headerlink" title="双端队列"></a>双端队列</h3><p>就是两端都可以出栈和入栈。</p><h2 id="应用-1"><a href="#应用-1" class="headerlink" title="应用"></a>应用</h2><p>解决主机与外设之间速度不匹配的问题，解决由多用户引起的cpu资源竞争问题。<br>eg：设置一个打印数据缓冲区。cpu中的任务排成一个队列。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;栈&quot;&gt;&lt;a href=&quot;#栈&quot; class=&quot;headerlink&quot; title=&quot;栈&quot;&gt;&lt;/a&gt;栈&lt;/h1&gt;&lt;h2 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h2&gt;&lt;p&gt;是线性表的应用</summary>
      
    
    
    
    <category term="数据结构" scheme="https://luoyongjia.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>【数据结构】线性表</title>
    <link href="https://luoyongjia.github.io/2021/03/08/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E7%BA%BF%E6%80%A7%E8%A1%A8/"/>
    <id>https://luoyongjia.github.io/2021/03/08/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E7%BA%BF%E6%80%A7%E8%A1%A8/</id>
    <published>2021-03-08T09:32:27.000Z</published>
    <updated>2021-03-08T15:00:48.968Z</updated>
    
    <content type="html"><![CDATA[<h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><p><strong>相同数据类型</strong>的数据元素，<strong>有限</strong>长的序列。是一种逻辑结构，表示元素之间一对一的相邻关系。</p><h1 id="顺序表"><a href="#顺序表" class="headerlink" title="顺序表"></a>顺序表</h1><p>使用连续地址存储。这里有一个<strong>位序</strong>，位序是从1开始的。</p><h2 id="分配空间"><a href="#分配空间" class="headerlink" title="分配空间"></a>分配空间</h2><h3 id="静态分配"><a href="#静态分配" class="headerlink" title="静态分配"></a>静态分配</h3><p>最开始的时候分配的一定量的空间，是不可以扩充的。</p><h3 id="动态分配"><a href="#动态分配" class="headerlink" title="动态分配"></a>动态分配</h3><p>开始分配一定量的空间。在存满之后，会开辟更大一个空间，然后替换之前那个小空间。</p><h2 id="基本操作的实现"><a href="#基本操作的实现" class="headerlink" title="基本操作的实现"></a>基本操作的实现</h2><p>纯属，只是写了，没有测试是否正确。VSCode的c++配置太难了，就这样吧。我试着去把JetBrains的工具下下来。果然白嫖就是有点难…</p><p>SeqList.h</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># ifndef CIRCLE_H</span><br><span class="line"># define CIRCLE_H</span><br><span class="line">int defaultSize &#x3D; 100;</span><br><span class="line">template &lt;typename T&gt;</span><br><span class="line">class SeqList</span><br><span class="line">&#123;</span><br><span class="line">private:</span><br><span class="line">    T *data;</span><br><span class="line">    int maxSize;</span><br><span class="line">    int last;</span><br><span class="line"></span><br><span class="line">public:</span><br><span class="line">    SeqList(int len &#x3D; defaultSize);</span><br><span class="line">    SeqList(const SeqList&lt;T&gt; &amp;L);   &#x2F;&#x2F; 拷贝构造</span><br><span class="line">    ~SeqList();</span><br><span class="line">    &#x2F;&#x2F; ostream &amp;operator&#x3D;(const SeqList&lt;T&gt; &amp;L);</span><br><span class="line"></span><br><span class="line">    bool Add(T elem);</span><br><span class="line">    bool Insert(int i, T elem);</span><br><span class="line">    bool Remove(int i, T elem);</span><br><span class="line">    int Search(T elem);</span><br><span class="line">    void SetData(int i, T elem);</span><br><span class="line">    void Show();</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">#endif</span><br></pre></td></tr></table></figure><p>SeqList.cpp</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &quot;SeqList.h&quot;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">template&lt;typename T&gt;</span><br><span class="line">SeqList&lt;T&gt;::SeqList(int len)&#123;</span><br><span class="line">    maxSize &#x3D; len;</span><br><span class="line">    data &#x3D; new T[maxSize];</span><br><span class="line">    last &#x3D; -1;  &#x2F;&#x2F;线性表为空</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">template&lt;typename T&gt;</span><br><span class="line">SeqList&lt;T&gt;::SeqList(const SeqList&lt;T&gt; &amp;L)&#123;</span><br><span class="line">    maxSize &#x3D; L.maxSize;</span><br><span class="line">    last &#x3D; L.last;</span><br><span class="line">    data &#x3D; new T[maxSize];</span><br><span class="line"></span><br><span class="line">    if(data &#x3D;&#x3D; NULL)&#123;</span><br><span class="line">        cout &lt;&lt; &quot;存储分配失败&quot; &lt;&lt; endl;</span><br><span class="line">        exit(1);        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">template&lt;typename T&gt;</span><br><span class="line">SeqList&lt;T&gt;::~SeqList()&#123;</span><br><span class="line">    delete[] data;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">template&lt;typename T&gt;</span><br><span class="line">bool SeqList&lt;T&gt;::Add(T elem)&#123;</span><br><span class="line">    if((last + 1)&gt;maxSize)</span><br><span class="line">        return false;</span><br><span class="line">    data[last++] &#x3D; elem;</span><br><span class="line">    return true;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">template&lt;typename T&gt;</span><br><span class="line">bool SeqList&lt;T&gt;::Insert(int i, T elem)&#123;</span><br><span class="line">    if(last &#x3D;&#x3D; maxSize-1)&#123;</span><br><span class="line">        &#x2F;&#x2F; 表满，不能再存</span><br><span class="line">        return false;</span><br><span class="line">    &#125;</span><br><span class="line">    if(i &lt; 0 || i &gt;last+1)&#123;</span><br><span class="line">        &#x2F;&#x2F; i越界了</span><br><span class="line">        return false;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    for (int j &#x3D; last; j &gt;&#x3D; i; j--)&#123;</span><br><span class="line">        data[j + 1] &#x3D; data[j];</span><br><span class="line">    &#125;</span><br><span class="line">    data[i] &#x3D; elem;</span><br><span class="line">    last++;</span><br><span class="line">    return true;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">template&lt;typename T&gt;</span><br><span class="line">bool SeqList&lt;T&gt;::Remove(int i, T elem)&#123;</span><br><span class="line">    if(i &lt; 0|| i &gt; last)</span><br><span class="line">        return false;</span><br><span class="line"></span><br><span class="line">    for (int j &#x3D; i; j &lt; last; j++)&#123;</span><br><span class="line">        data[j] &#x3D; data[j + 1];</span><br><span class="line">    &#125;</span><br><span class="line">    last--;</span><br><span class="line">    return true;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">template&lt;typename T&gt;</span><br><span class="line">int SeqList&lt;T&gt;::Search(T elem)&#123;</span><br><span class="line">    for (int i &#x3D; 0; i &lt; last; i++)&#123;</span><br><span class="line">        if(data[i] &#x3D;&#x3D; elem)</span><br><span class="line">            return true;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return false;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">template&lt;typename T&gt;</span><br><span class="line">void SeqList&lt;T&gt;::SetData(int i, T elem)&#123;</span><br><span class="line">    &#x2F;&#x2F; 检查i的合法性</span><br><span class="line">    if(i &lt; 0|| i &gt; last)&#123;</span><br><span class="line">        return;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    data[i] &#x3D; elem;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">template&lt;typename T&gt;</span><br><span class="line">void SeqList&lt;T&gt;::Show()&#123;</span><br><span class="line">    cout &lt;&lt; &quot;ok&quot; &lt;&lt; endl;</span><br><span class="line">    for (int i &#x3D; 0; i &lt; last; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        cout &lt;&lt; data[i] &lt;&lt; &quot; &quot;;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h2><p>插入： <script type="math/tex">O(n)</script><br>删除： <script type="math/tex">O(n)</script><br>查找： <script type="math/tex">O(1)</script><br>修改： <script type="math/tex">O(1)</script>    </p><h2 id="错题环节"><a href="#错题环节" class="headerlink" title="错题环节"></a>错题环节</h2><blockquote><ol><li>线性表的顺序存储结构是一种（ ）<br><br>A. 随机存取的存储结构        B. 顺序存取的存储结构  C. 索引存储的存储结构  D.散列存取的存储结构<br><br><br>⚠️选项中写的是<strong>存取</strong>。顺序存储，就是可以瞎取（按照下标直接存取），所以是随机的存取结构。而顺序的存取结构，是链式存储的，就是说只可以按照一定的顺序来存取。</li></ol></blockquote><h1 id="链式表示"><a href="#链式表示" class="headerlink" title="链式表示"></a>链式表示</h1><p>其实就是用链式存储来存顺序表，这其中不过是分配的空间不连续了。</p><h2 id="静态链表与动态链表"><a href="#静态链表与动态链表" class="headerlink" title="静态链表与动态链表"></a>静态链表与动态链表</h2><p>静态链表：初始化的时候，每个节点都初始化了。不能改变长短的。但是存储地址不一定是连续的。<br>动态链表：就，日常的链表</p><h2 id="链表的分类"><a href="#链表的分类" class="headerlink" title="链表的分类"></a>链表的分类</h2><h3 id="头与尾"><a href="#头与尾" class="headerlink" title="头与尾"></a>头与尾</h3><p>头节点与尾节点，可有可无。里面不存内容，单纯定个位。有时候计算链表的长度可以考虑下是否有头、尾的情况。</p><h3 id="单链表"><a href="#单链表" class="headerlink" title="单链表"></a>单链表</h3><p>日常的，单向的链表。</p><h3 id="双链表"><a href="#双链表" class="headerlink" title="双链表"></a>双链表</h3><p>就是不仅仅有指向下一个节点的指针，还有指向上一个节点的指针。</p><h3 id="循环链表"><a href="#循环链表" class="headerlink" title="循环链表"></a>循环链表</h3><p>尾部的next是头节点。就是，形成一个环了的链表。</p><h3 id="静态链表"><a href="#静态链表" class="headerlink" title="静态链表"></a>静态链表</h3><p>一个表，跟顺序表差不多。存的指针是节点的相对地址，也就是下一个元素的index。-1时结束。</p><p><img src="/2021/03/08/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E7%BA%BF%E6%80%A7%E8%A1%A8/1.jpg" alt="f1"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;相同数据类型&lt;/strong&gt;的数据元素，&lt;strong&gt;有限&lt;/strong&gt;长的序列。是一种逻辑结构，表示元素之间一对一</summary>
      
    
    
    
    <category term="数据结构" scheme="https://luoyongjia.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>503. 下一个更大的元素2</title>
    <link href="https://luoyongjia.github.io/2021/03/06/503-%E4%B8%8B%E4%B8%80%E4%B8%AA%E6%9B%B4%E5%A4%A7%E7%9A%84%E5%85%83%E7%B4%A02/"/>
    <id>https://luoyongjia.github.io/2021/03/06/503-%E4%B8%8B%E4%B8%80%E4%B8%AA%E6%9B%B4%E5%A4%A7%E7%9A%84%E5%85%83%E7%B4%A02/</id>
    <published>2021-03-06T15:37:15.000Z</published>
    <updated>2021-03-07T08:35:14.909Z</updated>
    
    <content type="html"><![CDATA[<p>先来看看题，这题…有点绕。</p><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>给定一个循环数组（首尾相接那种），输出每个元素的下一个更大元素。如果搜完了整个数组，都没有找到比它大的数字，就输出-1。</p><blockquote><p><strong>输入：</strong>[1, 2, 1]<br><br><strong>输出：</strong>[2, -1, 2]<br><br><strong>解释：</strong>第一个1的下一个更大的数是2；<br><br>数字2找不到下一个更大的数；<br>第二个1的下一个最大的数循环搜索，结果就是2</p></blockquote><h2 id="自己一想"><a href="#自己一想" class="headerlink" title="自己一想"></a>自己一想</h2><p>简单，粗暴的方法。这样时间复杂度到了<script type="math/tex">O(n^{2})</script></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">public int[] nextGreaterElements(int[] nums) &#123;</span><br><span class="line">    int[] ans &#x3D; new int[nums.length];</span><br><span class="line">    boolean forhead;</span><br><span class="line">    &#x2F;&#x2F; 找每一个数字的下一个更大的元素</span><br><span class="line">    for(int i &#x3D; 0; i &lt; nums.length; i++)&#123;</span><br><span class="line">        forhead &#x3D; true;</span><br><span class="line">        &#x2F;&#x2F; 先找i后面的元素</span><br><span class="line">        for(int j &#x3D; i+1; j &lt; nums.length; j++)&#123;</span><br><span class="line">            if(nums[j] &gt; nums[i])&#123;</span><br><span class="line">                ans[i] &#x3D; nums[j];</span><br><span class="line">                forhead &#x3D; false;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        &#x2F;&#x2F; 再找i前面的元素</span><br><span class="line">        if(forhead &#x3D;&#x3D; true)&#123;</span><br><span class="line">            int k;</span><br><span class="line">            for(k &#x3D; 0; k &lt; i; k++)&#123;</span><br><span class="line">                if(nums[k] &gt; nums[i])&#123;</span><br><span class="line">                    ans[i] &#x3D; nums[k];</span><br><span class="line">                    break;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            if(k &#x3D;&#x3D; i)&#123;</span><br><span class="line">                ans[i] &#x3D; -1;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="然后解析里的“单调栈”"><a href="#然后解析里的“单调栈”" class="headerlink" title="然后解析里的“单调栈”"></a>然后解析里的“单调栈”</h2><h3 id="单调栈"><a href="#单调栈" class="headerlink" title="单调栈"></a>单调栈</h3><p>栈里面的元素从栈底到栈顶是单调的。</p><ul><li>如果栈空，当前元素入栈</li><li>栈非空，判断当前和栈顶元素的大小<ul><li>如果比栈顶元素大，那么逐个弹出比较，构建ans</li><li>如果比栈顶元素小，则当前元素入栈</li></ul></li></ul><h3 id="循环数组实现方法："><a href="#循环数组实现方法：" class="headerlink" title="循环数组实现方法："></a>循环数组实现方法：</h3><p><strong>Way1:</strong>  把数组复制一份到数组的末尾<br><strong>Way2:</strong> 使用取模运算，把下表为<script type="math/tex">i</script>映射到数组<script type="math/tex">nums</script>的长度<script type="math/tex">0-N</script>中</p><h2 id="重写代码"><a href="#重写代码" class="headerlink" title="重写代码"></a>重写代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public int[] nextGreaterElements(int[] nums) &#123;</span><br><span class="line">    int n &#x3D; nums.length;</span><br><span class="line">    int[] ans &#x3D; new int[n];</span><br><span class="line">    &#x2F;&#x2F;将ans用-1初始化</span><br><span class="line">    Arrays.fill(ans, -1);</span><br><span class="line">    Deque&lt;Integer&gt; stack &#x3D; new LinkedList&lt;Integer&gt;();</span><br><span class="line">    for(int i &#x3D; 0; i &lt; n;i++)&#123;</span><br><span class="line">        &#x2F;&#x2F; 栈非空,且比栈顶元素大，逐个弹出比较，构建ans</span><br><span class="line">        while(!stack.isEmpty() &amp;&amp; nums[stack.peek()]&lt;nums[i%n])&#123;</span><br><span class="line">            ans[stack.pop()] &#x3D; nums[i%n];</span><br><span class="line">        &#125;</span><br><span class="line">        &#x2F;&#x2F; 不比栈顶元素大了，或者小了，或者栈空了，压入</span><br><span class="line">        stack.push(i%n);</span><br><span class="line">    &#125;</span><br><span class="line">    return ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="refer"><a href="#refer" class="headerlink" title="refer"></a>refer</h6><hr><p><a href="https://leetcode-cn.com/problems/next-greater-element-ii/solution/dong-hua-jiang-jie-dan-diao-zhan-by-fuxu-4z2g/">动画讲解：单调栈</a></p><p><a href="https://leetcode-cn.com/problems/next-greater-element-ii/solution/xia-yi-ge-geng-da-yuan-su-ii-by-leetcode-bwam/">code来源-LeetCode-Solution</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;先来看看题，这题…有点绕。&lt;/p&gt;
&lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;p&gt;给定一个循环数组（首尾相接那种），输出每个元素的下一个更大元素。如果搜完了整个数组，都没有找到比它</summary>
      
    
    
    
    <category term="leetcode" scheme="https://luoyongjia.github.io/categories/leetcode/"/>
    
    
    <category term="单调栈" scheme="https://luoyongjia.github.io/tags/%E5%8D%95%E8%B0%83%E6%A0%88/"/>
    
  </entry>
  
  <entry>
    <title>【数据结构】绪论</title>
    <link href="https://luoyongjia.github.io/2021/03/06/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E7%BB%AA%E8%AE%BA/"/>
    <id>https://luoyongjia.github.io/2021/03/06/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E7%BB%AA%E8%AE%BA/</id>
    <published>2021-03-06T12:27:22.000Z</published>
    <updated>2021-03-08T09:33:24.559Z</updated>
    
    <content type="html"><![CDATA[<p>我的数据结构是以《王道 2021数据结构复习指导》为基础。然后其中我认为难理解的点，会拎出来重点分析一下。</p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>很好，记不住的东西。</p><p><strong>数据</strong> 是信息，是数据，是程序直接处理的原料。<br><strong>数据元素</strong> 数据元素含有数据，是由一群数据组成的最小的、不可分割的最小单位。比如说，一个学生（包括他的姓名、学号、年龄等）就是一个数据元素。<br><strong>数据对象</strong> 一群同类型的数据元素组成的。比如说，学生对象，就是由一群学生对象组成的。<br><strong>数据类型</strong> 就比如说<code>byte, short, int, long</code>这些。就是一种数值的集合。其下面又有<strong>原子类型</strong>、<strong>结构类型</strong>、<strong>抽象数据类型</strong>，分别表示值不可分，值可以分，数据和与之相关的操作。<br><strong>数据结构</strong> 诶嘿，终于到它了。数据结构是有关系的数据元素的集合。包括<strong>逻辑结构</strong>，<strong>存储结构</strong>和<strong>数据的运算</strong>（个人理解为方法）。</p><h2 id="数据结构三要素"><a href="#数据结构三要素" class="headerlink" title="数据结构三要素"></a>数据结构三要素</h2><p>我觉得这里主要是在讲逻辑结构与存储结构。还有一个名词叫“物理结构”，我在这里将会把他们仨分清楚。</p><h3 id="逻辑结构"><a href="#逻辑结构" class="headerlink" title="逻辑结构"></a>逻辑结构</h3><p>逻辑逻辑嘛，那当然是一种抽象的东西，我们不考虑他们的实际应该怎么样，只去考虑这样子特性的数据有些什么特色，可以怎么操作。</p><p><img src="/2021/03/06/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E7%BB%AA%E8%AE%BA/逻辑结构.png" alt="f1"></p><p>这里要解释一下<strong>集合</strong>，就是一群数据挤到一起，没有其他联系这样子。</p><h3 id="存储结构（物理结构）"><a href="#存储结构（物理结构）" class="headerlink" title="存储结构（物理结构）"></a>存储结构（物理结构）</h3><p>就是解决如何将数据存储的问题。</p><p><img src="/2021/03/06/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E7%BB%AA%E8%AE%BA/存储结构.png" alt="f1"></p><h3 id="数据的运算"><a href="#数据的运算" class="headerlink" title="数据的运算"></a>数据的运算</h3><p>个人理解，就是操作、方法这种东西。</p><h3 id="这里来几道错题加深理解"><a href="#这里来几道错题加深理解" class="headerlink" title="这里来几道错题加深理解"></a>这里来几道错题加深理解</h3><blockquote><ol><li>可以用（ ）来定义一个完整的数据结构<br><br>A. 数据元素        B.数据对象        C.数据关系        D. 抽象数据类型<br><br>这里，👀上面的基本概念，就能得出答案D<br><br><br></li><li>以下属于逻辑结构的是（ ）<br><br>A. 顺序表    B. 哈希表    C. 有序表    D. 单链表<br><br><br>顺序表、哈希表、单链表，既可以表示逻辑结构，又可以表示存储结构。只有有序表是单纯的。<br><br><br></li><li>以下数据与数据的存储结构无关的术语是（ ）<br><br>A. 循环队列 B. 链表 C. 哈希表 D. 栈<br><br><br>B和C都很好排除。循环队列为一种数据结构？（这里存疑，之后来解决）可能是因为，他包含顺序表的逻辑结构，和顺序存储的存储结构，以及一些操作方法。讲真，<strong>不知道如何区分数据结构和逻辑结构的区别</strong>。问问后来补充。<br><br><br></li><li>在存储数据时，通常不仅要存储个数据元素的值，而且要存储( )<br><br>A. 数据的操作方法 B. 数据元素的类型 C. 数据元素之间的关系 D. 数据存取的方法<br><br><br>D，这没啥好说的，记住就好。</li></ol></blockquote><h2 id="算法评价"><a href="#算法评价" class="headerlink" title="算法评价"></a>算法评价</h2><p>算法，对于一个问题的解决方法逐步描述。</p><h3 id="一些性质"><a href="#一些性质" class="headerlink" title="一些性质"></a>一些性质</h3><p><strong>有穷性</strong> 是不是有穷时间能跑完<br><strong>确定性</strong> 描述是不是无歧义的<br><strong>可行性</strong> 电脑能不能跑起来<br><strong>I/O</strong> 输入输出<br><strong>正确性</strong> 能不能正确解决问题<br><strong>可读性</strong> 是不是很好理解<br><strong>健壮性</strong> 能不能面对攻击<br><strong>效率</strong> 效率就是效率，时间、空间效率</p><h3 id="时间复杂度、空间复杂度"><a href="#时间复杂度、空间复杂度" class="headerlink" title="时间复杂度、空间复杂度"></a>时间复杂度、空间复杂度</h3><p>这种主要考：给段代码，然后让算复杂度。</p><p>注意点：O(1)的可以是常数级别的复杂度。</p><h6 id="refer"><a href="#refer" class="headerlink" title="refer"></a>refer</h6><hr><p><a href="https://blog.csdn.net/YangTongA/article/details/78244252">数据结构之逻辑结构与物理结构（存储结构）</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;我的数据结构是以《王道 2021数据结构复习指导》为基础。然后其中我认为难理解的点，会拎出来重点分析一下。&lt;/p&gt;
&lt;h2 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h2&gt;&lt;p&gt;很</summary>
      
    
    
    
    <category term="数据结构" scheme="https://luoyongjia.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
  </entry>
  
</feed>
